{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers import Dense, Input, Embedding\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from wordcloud import WordCloud\n",
    "import gensim\n",
    "from gensim.models import word2vec\n",
    "import logging\n",
    "import  tensorflow as tf\n",
    "from collections import Counter\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import urllib.request\n",
    "import os\n",
    "import sys\n",
    "import zipfile\n",
    "import logging\n",
    "import pydot\n",
    "import graphviz\n",
    "import re\n",
    "import re\n",
    "from functools import reduce\n",
    "from collections import defaultdict\n",
    "import collections\n",
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "import wordsegment\n",
    "from wordsegment import load, segment\n",
    "from operator import itemgetter\n",
    "import keras\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('/users/rachelchen/downloads/conv.pkl', 'rb')\n",
    "\n",
    "# dump information to that file\n",
    "data = pickle.load(file)\n",
    "\n",
    "# close the file\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(data[:20000],columns=['Context','Utterance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sent):\n",
    "    '''Return the tokens of a sentence including punctuation.\n",
    "    >>> tokenize('Bob dropped the apple. Where is the apple?')\n",
    "    ['Bob', 'dropped', 'the', 'apple', '.', 'Where', 'is', 'the', 'apple', '?']\n",
    "    '''\n",
    "    return [x.strip() for x in re.findall(r\"[\\w']+|[.,!?]\", sent) if x.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(data):\n",
    "    questions = data.Context.apply(tokenize).values\n",
    "    answers = data.Utterance.apply(tokenize).values\n",
    "\n",
    "    #questions.apply(tokenize)\n",
    "    #answers.apply(tokenize)\n",
    "\n",
    "    data = [(q, a) for q, a in zip(questions, answers)]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = get_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Context</th>\n",
       "      <th>Utterance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>well , i thought we'd start with pronunciation...</td>\n",
       "      <td>not the hacking and gagging and spitting part ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>not the hacking and gagging and spitting part ...</td>\n",
       "      <td>okay . then how 'bout we try out some french c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>you're asking me out . that's so cute . what's...</td>\n",
       "      <td>forget it .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>no , no , it's my fault we didn't have a prope...</td>\n",
       "      <td>cameron . cameron .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cameron . cameron .</td>\n",
       "      <td>the thing is , cameron i'm at the mercy of a p...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Context  \\\n",
       "0  well , i thought we'd start with pronunciation...   \n",
       "1  not the hacking and gagging and spitting part ...   \n",
       "2  you're asking me out . that's so cute . what's...   \n",
       "3  no , no , it's my fault we didn't have a prope...   \n",
       "4                                cameron . cameron .   \n",
       "\n",
       "                                           Utterance  \n",
       "0  not the hacking and gagging and spitting part ...  \n",
       "1  okay . then how 'bout we try out some french c...  \n",
       "2                                        forget it .  \n",
       "3                                cameron . cameron .  \n",
       "4  the thing is , cameron i'm at the mercy of a p...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "APPO = {\n",
    "\"aren't\" : \"are not\",\n",
    "\"can't\" : \"cannot\",\n",
    "\"couldn't\" : \"could not\",\n",
    "\"didn't\" : \"did not\",\n",
    "\"doesn't\" : \"does not\",\n",
    "\"don't\" : \"do not\",\n",
    "\"hadn't\" : \"had not\",\n",
    "\"hasn't\" : \"has not\",\n",
    "\"haven't\" : \"have not\",\n",
    "\"he'd\" : \"he would\",\n",
    "\"he'll\" : \"he will\",\n",
    "\"he's\" : \"he is\",\n",
    "\"i'd\" : \"i would\",\n",
    "\"i'd\" : \"i had\",\n",
    "\"i'll\" : \"i will\",\n",
    "\"i'm\" : \"i am\",\n",
    "\"isn't\" : \"is not\",\n",
    "\"it's\" : \"it is\",\n",
    "\"it'll\":\"it will\",\n",
    "\"i've\" : \"i have\",\n",
    "\"let's\" : \"let us\",\n",
    "\"mightn't\" : \"might not\",\n",
    "\"mustn't\" : \"must not\",\n",
    "\"shan't\" : \"shall not\",\n",
    "\"she'd\" : \"she would\",\n",
    "\"she'll\" : \"she will\",\n",
    "\"she's\" : \"she is\",\n",
    "\"shouldn't\" : \"should not\",\n",
    "\"that's\" : \"that is\",\n",
    "\"there's\" : \"there is\",\n",
    "\"they'd\" : \"they would\",\n",
    "\"they'll\" : \"they will\",\n",
    "\"they're\" : \"they are\",\n",
    "\"they've\" : \"they have\",\n",
    "\"we'd\" : \"we would\",\n",
    "\"we're\" : \"we are\",\n",
    "\"weren't\" : \"were not\",\n",
    "\"we've\" : \"we have\",\n",
    "\"what'll\" : \"what will\",\n",
    "\"what're\" : \"what are\",\n",
    "\"what's\" : \"what is\",\n",
    "\"what've\" : \"what have\",\n",
    "\"where's\" : \"where is\",\n",
    "\"who'd\" : \"who would\",\n",
    "\"who'll\" : \"who will\",\n",
    "\"who're\" : \"who are\",\n",
    "\"who's\" : \"who is\",\n",
    "\"who've\" : \"who have\",\n",
    "\"won't\" : \"will not\",\n",
    "\"wouldn't\" : \"would not\",\n",
    "\"you'd\" : \"you would\",\n",
    "\"you'll\" : \"you will\",\n",
    "\"you're\" : \"you are\",\n",
    "\"you've\" : \"you have\",\n",
    "\"'re\": \" are\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'll\":\" will\",\n",
    "\"didn't\": \"did not\",\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(qa):\n",
    "    \n",
    "    question,answer = qa\n",
    "    w_q = [w.lower() for w in question if (len(w)!=1)|(w.isalpha())]\n",
    "    \n",
    "    w_a = [w.lower() for w in answer if (len(w)!=1)|(w.isalpha())]\n",
    "    w_q = [APPO[word] if word in APPO else word for word in w_q]\n",
    "    w_a = [APPO[word] if word in APPO else word for word in w_a]\n",
    "    q = \" \".join(w_q)\n",
    "    a = \" \".join(w_a)\n",
    "    return (q,a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = list(map(pre_process,train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(train,columns=['Context','Utterance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "input1 = data['Context'].fillna(\"\").tolist()\n",
    "output1 = data['Utterance'].fillna(\"\").tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = get_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = defaultdict(int)\n",
    "for question, answer in train:\n",
    "    for word in question + answer:\n",
    "        vocab[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11387"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import logging\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "pd.set_option('display.max_colwidth', 500)\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Context</th>\n",
       "      <th>Utterance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>well i thought we would start with pronunciation if that is okay with you</td>\n",
       "      <td>not the hacking and gagging and spitting part please</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>not the hacking and gagging and spitting part please</td>\n",
       "      <td>okay then how 'bout we try out some french cuisine saturday night</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>you are asking me out that is so cute what is your name again</td>\n",
       "      <td>forget it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>no no it is my fault we did not have a proper introduction</td>\n",
       "      <td>cameron cameron</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cameron cameron</td>\n",
       "      <td>the thing is cameron i am at the mercy of a particularly hideous breed of loser my sister i cannot date until she does</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>the thing is cameron i am at the mercy of a particularly hideous breed of loser my sister i cannot date until she does</td>\n",
       "      <td>seems like she could get a date easy enough</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>why why</td>\n",
       "      <td>unsolved mystery she used to be really popular when she started high school then it was just like she got sick of it or something</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>unsolved mystery she used to be really popular when she started high school then it was just like she got sick of it or something</td>\n",
       "      <td>that is a shame</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>gosh if only we could find kat a boyfriend</td>\n",
       "      <td>let me see what i can do</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>that is because it is such a nice one</td>\n",
       "      <td>forget french</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>how is our little find the wench a date plan progressing</td>\n",
       "      <td>well there is someone i think might be</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>there there</td>\n",
       "      <td>where where</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>you got something on your mind</td>\n",
       "      <td>i counted on you to help my cause you and that thug are obviously failing are not we ever going on our date</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>you have my word as a gentleman</td>\n",
       "      <td>you are sweet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>sure have</td>\n",
       "      <td>i really really really wanna go but i cannot not unless my sister goes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>i really really really wanna go but i cannot not unless my sister goes</td>\n",
       "      <td>i am workin' on it but she does not seem to be goin' for him</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>she is not a</td>\n",
       "      <td>lesbian no i found a picture of jared leto in one of her drawers so i am pretty sure she is not harboring same sex tendencies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>lesbian no i found a picture of jared leto in one of her drawers so i am pretty sure she is not harboring same sex tendencies</td>\n",
       "      <td>so that is the kind of guy she likes pretty ones</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>so that is the kind of guy she likes pretty ones</td>\n",
       "      <td>who knows all i have ever heard her say is that she would dip before dating a guy that smokes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>hi hi</td>\n",
       "      <td>looks like things worked out tonight huh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>you know chastity</td>\n",
       "      <td>i believe we share an art instructor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>have fun tonight</td>\n",
       "      <td>tons tons</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>i looked for you back at the party but you always seemed to be occupied</td>\n",
       "      <td>i was</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>i was</td>\n",
       "      <td>you never wanted to go out with 'me did you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>well no</td>\n",
       "      <td>then that is all you had to say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>then that is all you had to say</td>\n",
       "      <td>but but</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>but but</td>\n",
       "      <td>you always been this selfish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>do you listen to this crap</td>\n",
       "      <td>what crap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>i figured you would get to the good stuff eventually</td>\n",
       "      <td>what good stuff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>what good stuff</td>\n",
       "      <td>the real you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19970</th>\n",
       "      <td>prepare a tear harness for the female</td>\n",
       "      <td>no i swear i do not know please</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19971</th>\n",
       "      <td>no i swear i do not know please</td>\n",
       "      <td>do you think i am a fool that the commander does not know every bolt every weld of his ship</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19972</th>\n",
       "      <td>wait what did you say</td>\n",
       "      <td>please do not hurt them it is not their fault i am not the commander i do not know anything</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19973</th>\n",
       "      <td>explain</td>\n",
       "      <td>gwen the show there is no choice do it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19974</th>\n",
       "      <td>our dimwitted friends do not understand the concept of acting they have no theater no imagination these scientists</td>\n",
       "      <td>we pretend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19975</th>\n",
       "      <td>we pretend</td>\n",
       "      <td>simpler simpler</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19976</th>\n",
       "      <td>simpler simpler</td>\n",
       "      <td>we we lie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19977</th>\n",
       "      <td>we we lie</td>\n",
       "      <td>yes you understand that do not you mathesar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19978</th>\n",
       "      <td>accelerate to mark tommy</td>\n",
       "      <td>this is embarrassing really i shall not tell this story when i return home</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19979</th>\n",
       "      <td>right if this is about the thing tomorrow you can hammer out the details with my agent but make sure i have a limo from my house they jammed me into a toyota the last time i did one of these</td>\n",
       "      <td>i certainly but</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19980</th>\n",
       "      <td>i certainly but</td>\n",
       "      <td>catch me later okay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19981</th>\n",
       "      <td>commander welcome to the protector ii would you like to don your uniform</td>\n",
       "      <td>mind if we skip that i have to get back pretty quick for this thing in van nuys</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19982</th>\n",
       "      <td>mind if we skip that i have to get back pretty quick for this thing in van nuys</td>\n",
       "      <td>as you wish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19983</th>\n",
       "      <td>commander where are you going</td>\n",
       "      <td>home home</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19984</th>\n",
       "      <td>home home</td>\n",
       "      <td>you you mean earth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19985</th>\n",
       "      <td>you you mean earth</td>\n",
       "      <td>yeah earth time to get back to earth kids</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19986</th>\n",
       "      <td>but commander the negotiation you you you fired on him</td>\n",
       "      <td>right long live what is your planet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19987</th>\n",
       "      <td>right long live what is your planet</td>\n",
       "      <td>theramin theramin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19988</th>\n",
       "      <td>but what if sarris survives</td>\n",
       "      <td>oh i do not think so i gave him both barrels</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19989</th>\n",
       "      <td>oh i do not think so i gave him both barrels</td>\n",
       "      <td>he has a very powerful ship perhaps you would like to wait to see the results of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19990</th>\n",
       "      <td>an interstellar vox</td>\n",
       "      <td>thanks thanks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19991</th>\n",
       "      <td>how can we thank you commander you you have saved our people</td>\n",
       "      <td>it was a lot of fun you kids are great</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19992</th>\n",
       "      <td>weapons storage</td>\n",
       "      <td>it is perfectly safe i promise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19993</th>\n",
       "      <td>it is perfectly safe i promise</td>\n",
       "      <td>maintenance facility</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19994</th>\n",
       "      <td>to our brave guests few in this universe have the opportunity to meet their heroes we are blessed to count ourselves among them</td>\n",
       "      <td>wherever a distress signal sounds among the stars  will be there this fine ship this fine crew never give up never surrender</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19995</th>\n",
       "      <td>mathesar has sarris seen the historical records</td>\n",
       "      <td>no thank god he has not</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19996</th>\n",
       "      <td>no thank god he has not</td>\n",
       "      <td>then how did he find out about the device</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19997</th>\n",
       "      <td>then how did he find out about the device</td>\n",
       "      <td>our former commander was not strong</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19998</th>\n",
       "      <td>our former commander was not strong</td>\n",
       "      <td>former commander</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19999</th>\n",
       "      <td>former commander</td>\n",
       "      <td>i am sorry you deserve to be shown</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                              Context  \\\n",
       "0                                                                                                                           well i thought we would start with pronunciation if that is okay with you   \n",
       "1                                                                                                                                                not the hacking and gagging and spitting part please   \n",
       "2                                                                                                                                       you are asking me out that is so cute what is your name again   \n",
       "3                                                                                                                                          no no it is my fault we did not have a proper introduction   \n",
       "4                                                                                                                                                                                     cameron cameron   \n",
       "5                                                                              the thing is cameron i am at the mercy of a particularly hideous breed of loser my sister i cannot date until she does   \n",
       "6                                                                                                                                                                                             why why   \n",
       "7                                                                   unsolved mystery she used to be really popular when she started high school then it was just like she got sick of it or something   \n",
       "8                                                                                                                                                          gosh if only we could find kat a boyfriend   \n",
       "9                                                                                                                                                               that is because it is such a nice one   \n",
       "10                                                                                                                                           how is our little find the wench a date plan progressing   \n",
       "11                                                                                                                                                                                        there there   \n",
       "12                                                                                                                                                                     you got something on your mind   \n",
       "13                                                                                                                                                                    you have my word as a gentleman   \n",
       "14                                                                                                                                                                                          sure have   \n",
       "15                                                                                                                             i really really really wanna go but i cannot not unless my sister goes   \n",
       "16                                                                                                                                                                                       she is not a   \n",
       "17                                                                      lesbian no i found a picture of jared leto in one of her drawers so i am pretty sure she is not harboring same sex tendencies   \n",
       "18                                                                                                                                                   so that is the kind of guy she likes pretty ones   \n",
       "19                                                                                                                                                                                              hi hi   \n",
       "20                                                                                                                                                                                  you know chastity   \n",
       "21                                                                                                                                                                                   have fun tonight   \n",
       "22                                                                                                                            i looked for you back at the party but you always seemed to be occupied   \n",
       "23                                                                                                                                                                                              i was   \n",
       "24                                                                                                                                                                                            well no   \n",
       "25                                                                                                                                                                    then that is all you had to say   \n",
       "26                                                                                                                                                                                            but but   \n",
       "27                                                                                                                                                                         do you listen to this crap   \n",
       "28                                                                                                                                               i figured you would get to the good stuff eventually   \n",
       "29                                                                                                                                                                                    what good stuff   \n",
       "...                                                                                                                                                                                               ...   \n",
       "19970                                                                                                                                                           prepare a tear harness for the female   \n",
       "19971                                                                                                                                                                 no i swear i do not know please   \n",
       "19972                                                                                                                                                                           wait what did you say   \n",
       "19973                                                                                                                                                                                         explain   \n",
       "19974                                                                              our dimwitted friends do not understand the concept of acting they have no theater no imagination these scientists   \n",
       "19975                                                                                                                                                                                      we pretend   \n",
       "19976                                                                                                                                                                                 simpler simpler   \n",
       "19977                                                                                                                                                                                       we we lie   \n",
       "19978                                                                                                                                                                        accelerate to mark tommy   \n",
       "19979  right if this is about the thing tomorrow you can hammer out the details with my agent but make sure i have a limo from my house they jammed me into a toyota the last time i did one of these   \n",
       "19980                                                                                                                                                                                 i certainly but   \n",
       "19981                                                                                                                        commander welcome to the protector ii would you like to don your uniform   \n",
       "19982                                                                                                                 mind if we skip that i have to get back pretty quick for this thing in van nuys   \n",
       "19983                                                                                                                                                                   commander where are you going   \n",
       "19984                                                                                                                                                                                       home home   \n",
       "19985                                                                                                                                                                              you you mean earth   \n",
       "19986                                                                                                                                          but commander the negotiation you you you fired on him   \n",
       "19987                                                                                                                                                             right long live what is your planet   \n",
       "19988                                                                                                                                                                     but what if sarris survives   \n",
       "19989                                                                                                                                                    oh i do not think so i gave him both barrels   \n",
       "19990                                                                                                                                                                             an interstellar vox   \n",
       "19991                                                                                                                                    how can we thank you commander you you have saved our people   \n",
       "19992                                                                                                                                                                                 weapons storage   \n",
       "19993                                                                                                                                                                  it is perfectly safe i promise   \n",
       "19994                                                                 to our brave guests few in this universe have the opportunity to meet their heroes we are blessed to count ourselves among them   \n",
       "19995                                                                                                                                                 mathesar has sarris seen the historical records   \n",
       "19996                                                                                                                                                                         no thank god he has not   \n",
       "19997                                                                                                                                                       then how did he find out about the device   \n",
       "19998                                                                                                                                                             our former commander was not strong   \n",
       "19999                                                                                                                                                                                former commander   \n",
       "\n",
       "                                                                                                                               Utterance  \n",
       "0                                                                                   not the hacking and gagging and spitting part please  \n",
       "1                                                                      okay then how 'bout we try out some french cuisine saturday night  \n",
       "2                                                                                                                              forget it  \n",
       "3                                                                                                                        cameron cameron  \n",
       "4                 the thing is cameron i am at the mercy of a particularly hideous breed of loser my sister i cannot date until she does  \n",
       "5                                                                                            seems like she could get a date easy enough  \n",
       "6      unsolved mystery she used to be really popular when she started high school then it was just like she got sick of it or something  \n",
       "7                                                                                                                        that is a shame  \n",
       "8                                                                                                               let me see what i can do  \n",
       "9                                                                                                                          forget french  \n",
       "10                                                                                                well there is someone i think might be  \n",
       "11                                                                                                                           where where  \n",
       "12                           i counted on you to help my cause you and that thug are obviously failing are not we ever going on our date  \n",
       "13                                                                                                                         you are sweet  \n",
       "14                                                                i really really really wanna go but i cannot not unless my sister goes  \n",
       "15                                                                          i am workin' on it but she does not seem to be goin' for him  \n",
       "16         lesbian no i found a picture of jared leto in one of her drawers so i am pretty sure she is not harboring same sex tendencies  \n",
       "17                                                                                      so that is the kind of guy she likes pretty ones  \n",
       "18                                         who knows all i have ever heard her say is that she would dip before dating a guy that smokes  \n",
       "19                                                                                              looks like things worked out tonight huh  \n",
       "20                                                                                                  i believe we share an art instructor  \n",
       "21                                                                                                                             tons tons  \n",
       "22                                                                                                                                 i was  \n",
       "23                                                                                           you never wanted to go out with 'me did you  \n",
       "24                                                                                                       then that is all you had to say  \n",
       "25                                                                                                                               but but  \n",
       "26                                                                                                          you always been this selfish  \n",
       "27                                                                                                                             what crap  \n",
       "28                                                                                                                       what good stuff  \n",
       "29                                                                                                                          the real you  \n",
       "...                                                                                                                                  ...  \n",
       "19970                                                                                                    no i swear i do not know please  \n",
       "19971                                        do you think i am a fool that the commander does not know every bolt every weld of his ship  \n",
       "19972                                        please do not hurt them it is not their fault i am not the commander i do not know anything  \n",
       "19973                                                                                             gwen the show there is no choice do it  \n",
       "19974                                                                                                                         we pretend  \n",
       "19975                                                                                                                    simpler simpler  \n",
       "19976                                                                                                                          we we lie  \n",
       "19977                                                                                        yes you understand that do not you mathesar  \n",
       "19978                                                         this is embarrassing really i shall not tell this story when i return home  \n",
       "19979                                                                                                                    i certainly but  \n",
       "19980                                                                                                                catch me later okay  \n",
       "19981                                                    mind if we skip that i have to get back pretty quick for this thing in van nuys  \n",
       "19982                                                                                                                        as you wish  \n",
       "19983                                                                                                                          home home  \n",
       "19984                                                                                                                 you you mean earth  \n",
       "19985                                                                                          yeah earth time to get back to earth kids  \n",
       "19986                                                                                                right long live what is your planet  \n",
       "19987                                                                                                                  theramin theramin  \n",
       "19988                                                                                       oh i do not think so i gave him both barrels  \n",
       "19989                                                   he has a very powerful ship perhaps you would like to wait to see the results of  \n",
       "19990                                                                                                                      thanks thanks  \n",
       "19991                                                                                             it was a lot of fun you kids are great  \n",
       "19992                                                                                                     it is perfectly safe i promise  \n",
       "19993                                                                                                               maintenance facility  \n",
       "19994       wherever a distress signal sounds among the stars  will be there this fine ship this fine crew never give up never surrender  \n",
       "19995                                                                                                            no thank god he has not  \n",
       "19996                                                                                          then how did he find out about the device  \n",
       "19997                                                                                                our former commander was not strong  \n",
       "19998                                                                                                                   former commander  \n",
       "19999                                                                                                 i am sorry you deserve to be shown  \n",
       "\n",
       "[20000 rows x 2 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf, testdf = train_test_split(data, \n",
    "                                   test_size=.10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 18,000 rows 2 columns\n",
      "Test: 2,000 rows 2 columns\n"
     ]
    }
   ],
   "source": [
    "print(f'Train: {traindf.shape[0]:,} rows {traindf.shape[1]:,} columns')\n",
    "print(f'Test: {testdf.shape[0]:,} rows {testdf.shape[1]:,} columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Context</th>\n",
       "      <th>Utterance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17683</th>\n",
       "      <td>what is doug do in chicago</td>\n",
       "      <td>he works for the airline he will be out here you will meet him</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12340</th>\n",
       "      <td>of course</td>\n",
       "      <td>well lately some of the memories have begun to come back and i had kinda like to talk to you about one of them in particular it'd be a big help</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17357</th>\n",
       "      <td>uh sue i cannot</td>\n",
       "      <td>what what do you mean you</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Context  \\\n",
       "17683  what is doug do in chicago   \n",
       "12340                   of course   \n",
       "17357             uh sue i cannot   \n",
       "\n",
       "                                                                                                                                             Utterance  \n",
       "17683                                                                                   he works for the airline he will be out here you will meet him  \n",
       "12340  well lately some of the memories have begun to come back and i had kinda like to talk to you about one of them in particular it'd be a big help  \n",
       "17357                                                                                                                        what what do you mean you  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traindf.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_context_raw = traindf.Context.tolist()\n",
    "train_utterance_raw = traindf.Utterance.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'what is doug do in chicago'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_context_raw[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "from ktext.preprocess import processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:....tokenizing data\n",
      "WARNING:root:(1/2) done. 4 sec\n",
      "WARNING:root:....building corpus\n",
      "WARNING:root:(2/2) done. 0 sec\n",
      "WARNING:root:Finished parsing 18,000 documents.\n",
      "WARNING:root:...fit is finished, beginning transform\n",
      "WARNING:root:...padding data\n",
      "WARNING:root:done. 0 sec\n"
     ]
    }
   ],
   "source": [
    "context_pp = processor(keep_n=8000, padding_maxlen=70)\n",
    "train_context_vecs = context_pp.fit_transform(train_context_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "original string:\n",
      " what is doug do in chicago \n",
      "\n",
      "after pre-processing:\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0   11    5 1539   10   18 2801] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('\\noriginal string:\\n', train_context_raw[0], '\\n')\n",
    "print('after pre-processing:\\n', train_context_vecs[0], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:....tokenizing data\n",
      "WARNING:root:(1/2) done. 6 sec\n",
      "WARNING:root:....building corpus\n",
      "WARNING:root:(2/2) done. 0 sec\n",
      "WARNING:root:Finished parsing 18,000 documents.\n",
      "WARNING:root:...fit is finished, beginning transform\n",
      "WARNING:root:...padding data\n",
      "WARNING:root:done. 0 sec\n"
     ]
    }
   ],
   "source": [
    "utterance_pp = processor(append_indicators=True, keep_n=4500, \n",
    "                     padding_maxlen=12, padding ='post')\n",
    "\n",
    "# process the title data\n",
    "train_utterance_vecs = utterance_pp.fit_transform(train_utterance_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "original string:\n",
      " he works for the airline he will be out here you will meet him\n",
      "after pre-processing:\n",
      " [  2  22 602  27   6   1  22  24  32  51  47   5]\n"
     ]
    }
   ],
   "source": [
    "print('\\noriginal string:\\n', train_utterance_raw[0])\n",
    "print('after pre-processing:\\n', train_utterance_vecs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import dill as dpickle\n",
    "import numpy as np\n",
    "\n",
    "# Save the preprocessor\n",
    "with open('body_pp.dpkl', 'wb') as f:\n",
    "    dpickle.dump(context_pp, f)\n",
    "\n",
    "with open('title_pp.dpkl', 'wb') as f:\n",
    "    dpickle.dump(utterance_pp, f)\n",
    "\n",
    "# Save the processed data\n",
    "np.save('train_title_vecs.npy', train_utterance_vecs)\n",
    "np.save('train_body_vecs.npy', train_context_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "from IPython.display import SVG, display\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "import logging\n",
    "import numpy as np\n",
    "import dill as dpickle\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from random import random\n",
    "from nltk.translate.bleu_score import corpus_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_text_processor(fname='title_pp.dpkl'):\n",
    "    \"\"\"\n",
    "    Load preprocessors from disk.\n",
    "    Parameters\n",
    "    ----------\n",
    "    fname: str\n",
    "        file name of ktext.proccessor object\n",
    "    Returns\n",
    "    -------\n",
    "    num_tokens : int\n",
    "        size of vocabulary loaded into ktext.processor\n",
    "    pp : ktext.processor\n",
    "        the processor you are trying to load\n",
    "    Typical Usage:\n",
    "    -------------\n",
    "    num_decoder_tokens, title_pp = load_text_processor(fname='title_pp.dpkl')\n",
    "    num_encoder_tokens, body_pp = load_text_processor(fname='body_pp.dpkl')\n",
    "    \"\"\"\n",
    "    # Load files from disk\n",
    "    with open(fname, 'rb') as f:\n",
    "        pp = dpickle.load(f)\n",
    "\n",
    "    num_tokens = max(pp.id2token.keys()) + 1\n",
    "    print(f'Size of vocabulary for {fname}: {num_tokens:,}')\n",
    "    return num_tokens, pp\n",
    "\n",
    "\n",
    "def load_decoder_inputs(decoder_np_vecs='train_title_vecs.npy'):\n",
    "    \"\"\"\n",
    "    Load decoder inputs.\n",
    "    Parameters\n",
    "    ----------\n",
    "    decoder_np_vecs : str\n",
    "        filename of serialized numpy.array of decoder input (issue title)\n",
    "    Returns\n",
    "    -------\n",
    "    decoder_input_data : numpy.array\n",
    "        The data fed to the decoder as input during training for teacher forcing.\n",
    "        This is the same as `decoder_np_vecs` except the last position.\n",
    "    decoder_target_data : numpy.array\n",
    "        The data that the decoder data is trained to generate (issue title).\n",
    "        Calculated by sliding `decoder_np_vecs` one position forward.\n",
    "    \"\"\"\n",
    "    vectorized_title = np.load(decoder_np_vecs)\n",
    "    # For Decoder Input, you don't need the last word as that is only for prediction\n",
    "    # when we are training using Teacher Forcing.\n",
    "    decoder_input_data = vectorized_title[:, :-1]\n",
    "\n",
    "    # Decoder Target Data Is Ahead By 1 Time Step From Decoder Input Data (Teacher Forcing)\n",
    "    decoder_target_data = vectorized_title[:, 1:]\n",
    "\n",
    "    print(f'Shape of decoder input: {decoder_input_data.shape}')\n",
    "    print(f'Shape of decoder target: {decoder_target_data.shape}')\n",
    "    return decoder_input_data, decoder_target_data\n",
    "\n",
    "\n",
    "def load_encoder_inputs(encoder_np_vecs='train_body_vecs.npy'):\n",
    "    \"\"\"\n",
    "    Load variables & data that are inputs to encoder.\n",
    "    Parameters\n",
    "    ----------\n",
    "    encoder_np_vecs : str\n",
    "        filename of serialized numpy.array of encoder input (issue title)\n",
    "    Returns\n",
    "    -------\n",
    "    encoder_input_data : numpy.array\n",
    "        The issue body\n",
    "    doc_length : int\n",
    "        The standard document length of the input for the encoder after padding\n",
    "        the shape of this array will be (num_examples, doc_length)\n",
    "    \"\"\"\n",
    "    vectorized_body = np.load(encoder_np_vecs)\n",
    "    # Encoder input is simply the body of the issue text\n",
    "    encoder_input_data = vectorized_body\n",
    "    doc_length = encoder_input_data.shape[1]\n",
    "    print(f'Shape of encoder input: {encoder_input_data.shape}')\n",
    "    return encoder_input_data, doc_length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of encoder input: (18000, 70)\n",
      "Shape of decoder input: (18000, 11)\n",
      "Shape of decoder target: (18000, 11)\n"
     ]
    }
   ],
   "source": [
    "encoder_input_data, doc_length = load_encoder_inputs('train_body_vecs.npy')\n",
    "decoder_input_data, decoder_target_data = load_decoder_inputs('train_title_vecs.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vocabulary for body_pp.dpkl: 8,002\n",
      "Size of vocabulary for title_pp.dpkl: 4,502\n"
     ]
    }
   ],
   "source": [
    "num_encoder_tokens, context_pp = load_text_processor('body_pp.dpkl')\n",
    "num_decoder_tokens, utterance_pp = load_text_processor('title_pp.dpkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, GRU, Dense, Embedding, Bidirectional, BatchNormalization\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#arbitrarly set latent dimension for embedding and hidden units\n",
    "latent_dim = 300\n",
    "\n",
    "##### Define Model Architecture ######\n",
    "\n",
    "########################\n",
    "#### Encoder Model ####\n",
    "encoder_inputs = Input(shape=(doc_length,), name='Encoder-Input')\n",
    "\n",
    "# Word embeding for encoder (ex: Issue Body)\n",
    "x = Embedding(num_encoder_tokens, latent_dim, name='Body-Word-Embedding', mask_zero=False)(encoder_inputs)\n",
    "x = BatchNormalization(name='Encoder-Batchnorm-1')(x)\n",
    "\n",
    "# Intermediate GRU layer (optional)\n",
    "#x = GRU(latent_dim, name='Encoder-Intermediate-GRU', return_sequences=True)(x)\n",
    "#x = BatchNormalization(name='Encoder-Batchnorm-2')(x)\n",
    "\n",
    "# We do not need the `encoder_output` just the hidden state.\n",
    "_, state_h = GRU(latent_dim, return_state=True, name='Encoder-Last-GRU')(x)\n",
    "\n",
    "# Encapsulate the encoder as a separate entity so we can just \n",
    "#  encode without decoding if we want to.\n",
    "encoder_model = Model(inputs=encoder_inputs, outputs=state_h, name='Encoder-Model')\n",
    "\n",
    "seq2seq_encoder_out = encoder_model(encoder_inputs)\n",
    "\n",
    "########################\n",
    "#### Decoder Model ####\n",
    "decoder_inputs = Input(shape=(None,), name='Decoder-Input')  # for teacher forcing\n",
    "\n",
    "# Word Embedding For Decoder (ex: Issue Titles)\n",
    "dec_emb = Embedding(num_decoder_tokens, latent_dim, name='Decoder-Word-Embedding', mask_zero=False)(decoder_inputs)\n",
    "dec_bn = BatchNormalization(name='Decoder-Batchnorm-1')(dec_emb)\n",
    "\n",
    "# Set up the decoder, using `decoder_state_input` as initial state.\n",
    "decoder_gru = GRU(latent_dim, return_state=True, return_sequences=True, name='Decoder-GRU')\n",
    "decoder_gru_output, _ = decoder_gru(dec_bn, initial_state=seq2seq_encoder_out)\n",
    "x = BatchNormalization(name='Decoder-Batchnorm-2')(decoder_gru_output)\n",
    "\n",
    "# Dense layer for prediction\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax', name='Final-Output-Dense')\n",
    "decoder_outputs = decoder_dense(x)\n",
    "\n",
    "########################\n",
    "#### Seq2Seq Model ####\n",
    "\n",
    "#seq2seq_decoder_out = decoder_model([decoder_inputs, seq2seq_encoder_out])\n",
    "seq2seq_Model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "\n",
    "seq2seq_Model.compile(optimizer=optimizers.Nadam(lr=0.001), loss='sparse_categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viz_model_architecture(model):\n",
    "    \"\"\"Visualize model architecture in Jupyter notebook.\"\"\"\n",
    "    display(SVG(model_to_dot(model).create(prog='dot', format='svg')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Decoder-Input (InputLayer)      (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-Word-Embedding (Embeddi (None, None, 300)    1350600     Decoder-Input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-Input (InputLayer)      (None, 70)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-Batchnorm-1 (BatchNorma (None, None, 300)    1200        Decoder-Word-Embedding[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-Model (Model)           (None, 300)          2942700     Encoder-Input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-GRU (GRU)               [(None, None, 300),  540900      Decoder-Batchnorm-1[0][0]        \n",
      "                                                                 Encoder-Model[1][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-Batchnorm-2 (BatchNorma (None, None, 300)    1200        Decoder-GRU[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Final-Output-Dense (Dense)      (None, None, 4502)   1355102     Decoder-Batchnorm-2[0][0]        \n",
      "==================================================================================================\n",
      "Total params: 6,191,702\n",
      "Trainable params: 6,189,902\n",
      "Non-trainable params: 1,800\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"410pt\" viewBox=\"0.00 0.00 445.37 410.00\" width=\"445pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 406)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"#ffffff\" points=\"-4,4 -4,-406 441.3721,-406 441.3721,4 -4,4\" stroke=\"transparent\"/>\n",
       "<!-- 4877267408 -->\n",
       "<g class=\"node\" id=\"node1\">\n",
       "<title>4877267408</title>\n",
       "<polygon fill=\"none\" points=\"47.8208,-365.5 47.8208,-401.5 214.7583,-401.5 214.7583,-365.5 47.8208,-365.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"131.2896\" y=\"-379.3\">Decoder-Input: InputLayer</text>\n",
       "</g>\n",
       "<!-- 4343047392 -->\n",
       "<g class=\"node\" id=\"node2\">\n",
       "<title>4343047392</title>\n",
       "<polygon fill=\"none\" points=\"11.4365,-292.5 11.4365,-328.5 251.1426,-328.5 251.1426,-292.5 11.4365,-292.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"131.2896\" y=\"-306.3\">Decoder-Word-Embedding: Embedding</text>\n",
       "</g>\n",
       "<!-- 4877267408&#45;&gt;4343047392 -->\n",
       "<g class=\"edge\" id=\"edge1\">\n",
       "<title>4877267408-&gt;4343047392</title>\n",
       "<path d=\"M131.2896,-365.4551C131.2896,-357.3828 131.2896,-347.6764 131.2896,-338.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"134.7896,-338.5903 131.2896,-328.5904 127.7896,-338.5904 134.7896,-338.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 4853771960 -->\n",
       "<g class=\"node\" id=\"node4\">\n",
       "<title>4853771960</title>\n",
       "<polygon fill=\"none\" points=\"0,-219.5 0,-255.5 262.5791,-255.5 262.5791,-219.5 0,-219.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"131.2896\" y=\"-233.3\">Decoder-Batchnorm-1: BatchNormalization</text>\n",
       "</g>\n",
       "<!-- 4343047392&#45;&gt;4853771960 -->\n",
       "<g class=\"edge\" id=\"edge2\">\n",
       "<title>4343047392-&gt;4853771960</title>\n",
       "<path d=\"M131.2896,-292.4551C131.2896,-284.3828 131.2896,-274.6764 131.2896,-265.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"134.7896,-265.5903 131.2896,-255.5904 127.7896,-265.5904 134.7896,-265.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 4853910216 -->\n",
       "<g class=\"node\" id=\"node3\">\n",
       "<title>4853910216</title>\n",
       "<polygon fill=\"none\" points=\"271.207,-292.5 271.207,-328.5 437.3721,-328.5 437.3721,-292.5 271.207,-292.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"354.2896\" y=\"-306.3\">Encoder-Input: InputLayer</text>\n",
       "</g>\n",
       "<!-- 4877737656 -->\n",
       "<g class=\"node\" id=\"node5\">\n",
       "<title>4877737656</title>\n",
       "<polygon fill=\"none\" points=\"280.5278,-219.5 280.5278,-255.5 428.0513,-255.5 428.0513,-219.5 280.5278,-219.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"354.2896\" y=\"-233.3\">Encoder-Model: Model</text>\n",
       "</g>\n",
       "<!-- 4853910216&#45;&gt;4877737656 -->\n",
       "<g class=\"edge\" id=\"edge3\">\n",
       "<title>4853910216-&gt;4877737656</title>\n",
       "<path d=\"M354.2896,-292.4551C354.2896,-284.3828 354.2896,-274.6764 354.2896,-265.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"357.7896,-265.5903 354.2896,-255.5904 350.7896,-265.5904 357.7896,-265.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 4877711792 -->\n",
       "<g class=\"node\" id=\"node6\">\n",
       "<title>4877711792</title>\n",
       "<polygon fill=\"none\" points=\"175.1348,-146.5 175.1348,-182.5 309.4443,-182.5 309.4443,-146.5 175.1348,-146.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"242.2896\" y=\"-160.3\">Decoder-GRU: GRU</text>\n",
       "</g>\n",
       "<!-- 4853771960&#45;&gt;4877711792 -->\n",
       "<g class=\"edge\" id=\"edge4\">\n",
       "<title>4853771960-&gt;4877711792</title>\n",
       "<path d=\"M158.7278,-219.4551C173.0032,-210.0667 190.6399,-198.4678 206.0258,-188.3491\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"208.3504,-191.0095 214.7823,-182.5904 204.504,-185.1609 208.3504,-191.0095\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 4877737656&#45;&gt;4877711792 -->\n",
       "<g class=\"edge\" id=\"edge5\">\n",
       "<title>4877737656-&gt;4877711792</title>\n",
       "<path d=\"M326.6042,-219.4551C312.0655,-209.979 294.0716,-198.2508 278.4453,-188.0658\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"280.3334,-185.1186 270.0446,-182.5904 276.5111,-190.983 280.3334,-185.1186\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 4878415240 -->\n",
       "<g class=\"node\" id=\"node7\">\n",
       "<title>4878415240</title>\n",
       "<polygon fill=\"none\" points=\"111,-73.5 111,-109.5 373.5791,-109.5 373.5791,-73.5 111,-73.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"242.2896\" y=\"-87.3\">Decoder-Batchnorm-2: BatchNormalization</text>\n",
       "</g>\n",
       "<!-- 4877711792&#45;&gt;4878415240 -->\n",
       "<g class=\"edge\" id=\"edge6\">\n",
       "<title>4877711792-&gt;4878415240</title>\n",
       "<path d=\"M242.2896,-146.4551C242.2896,-138.3828 242.2896,-128.6764 242.2896,-119.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"245.7896,-119.5903 242.2896,-109.5904 238.7896,-119.5904 245.7896,-119.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 4878425560 -->\n",
       "<g class=\"node\" id=\"node8\">\n",
       "<title>4878425560</title>\n",
       "<polygon fill=\"none\" points=\"157.1118,-.5 157.1118,-36.5 327.4673,-36.5 327.4673,-.5 157.1118,-.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"242.2896\" y=\"-14.3\">Final-Output-Dense: Dense</text>\n",
       "</g>\n",
       "<!-- 4878415240&#45;&gt;4878425560 -->\n",
       "<g class=\"edge\" id=\"edge7\">\n",
       "<title>4878415240-&gt;4878425560</title>\n",
       "<path d=\"M242.2896,-73.4551C242.2896,-65.3828 242.2896,-55.6764 242.2896,-46.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"245.7896,-46.5903 242.2896,-36.5904 238.7896,-46.5904 245.7896,-46.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "seq2seq_Model.summary()\n",
    "viz_model_architecture(seq2seq_Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15840 samples, validate on 2160 samples\n",
      "Epoch 1/7\n",
      "15840/15840 [==============================] - 388s 25ms/step - loss: 6.1814 - val_loss: 3.9811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer Decoder-GRU was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'Encoder-Model/Encoder-Last-GRU/while/Exit_3:0' shape=(?, 300) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/7\n",
      "15840/15840 [==============================] - 361s 23ms/step - loss: 4.0355 - val_loss: 3.5841\n",
      "Epoch 3/7\n",
      "15840/15840 [==============================] - 310s 20ms/step - loss: 3.4668 - val_loss: 3.5865\n",
      "Epoch 4/7\n",
      "15840/15840 [==============================] - 306s 19ms/step - loss: 3.2316 - val_loss: 3.5538\n",
      "Epoch 5/7\n",
      "15840/15840 [==============================] - 313s 20ms/step - loss: 3.0498 - val_loss: 3.5629\n",
      "Epoch 6/7\n",
      "15840/15840 [==============================] - 307s 19ms/step - loss: 2.8859 - val_loss: 3.5203\n",
      "Epoch 7/7\n",
      "15840/15840 [==============================] - 350s 22ms/step - loss: 2.7186 - val_loss: 3.4659\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import CSVLogger, ModelCheckpoint\n",
    "\n",
    "script_name_base = 'tutorial_seq2seq'\n",
    "csv_logger = CSVLogger('{:}.log'.format(script_name_base))\n",
    "model_checkpoint = ModelCheckpoint('{:}.epoch{{epoch:02d}}-val{{val_loss:.5f}}.hdf5'.format(script_name_base),\n",
    "                                   save_best_only=True)\n",
    "\n",
    "batch_size = 1200\n",
    "epochs = 7\n",
    "history = seq2seq_Model.fit([encoder_input_data, decoder_input_data], np.expand_dims(decoder_target_data, -1),\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.12, callbacks=[csv_logger, model_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq_Inference(object):\n",
    "    def __init__(self,\n",
    "                 encoder_preprocessor,\n",
    "                 decoder_preprocessor,\n",
    "                 seq2seq_model):\n",
    "\n",
    "        self.pp_body = encoder_preprocessor\n",
    "        self.pp_title = decoder_preprocessor\n",
    "        self.seq2seq_model = seq2seq_model\n",
    "        self.encoder_model = extract_encoder_model(seq2seq_model)\n",
    "        self.decoder_model = extract_decoder_model(seq2seq_model)\n",
    "        self.default_max_len_title = self.pp_title.padding_maxlen\n",
    "        self.nn = None\n",
    "        self.rec_df = None\n",
    "\n",
    "    def generate_issue_title(self,\n",
    "                             raw_input_text,\n",
    "                             max_len_title=None):\n",
    "        \"\"\"\n",
    "        Use the seq2seq model to generate a title given the body of an issue.\n",
    "        Inputs\n",
    "        ------\n",
    "        raw_input: str\n",
    "            The body of the issue text as an input string\n",
    "        max_len_title: int (optional)\n",
    "            The maximum length of the title the model will generate\n",
    "        \"\"\"\n",
    "        if max_len_title is None:\n",
    "            max_len_title = self.default_max_len_title\n",
    "        # get the encoder's features for the decoder\n",
    "        raw_tokenized = self.pp_body.transform([raw_input_text])\n",
    "        body_encoding = self.encoder_model.predict(raw_tokenized)\n",
    "        # we want to save the encoder's embedding before its updated by decoder\n",
    "        #   because we can use that as an embedding for other tasks.\n",
    "        original_body_encoding = body_encoding\n",
    "        state_value = np.array(self.pp_title.token2id['_start_']).reshape(1, 1)\n",
    "\n",
    "        decoded_sentence = []\n",
    "        stop_condition = False\n",
    "        while not stop_condition:\n",
    "            preds, st = self.decoder_model.predict([state_value, body_encoding])\n",
    "\n",
    "            # We are going to ignore indices 0 (padding) and indices 1 (unknown)\n",
    "            # Argmax will return the integer index corresponding to the\n",
    "            #  prediction + 2 b/c we chopped off first two\n",
    "            pred_idx = np.argmax(preds[:, :, 2:]) + 2\n",
    "\n",
    "            # retrieve word from index prediction\n",
    "            pred_word_str = self.pp_title.id2token[pred_idx]\n",
    "\n",
    "            if pred_word_str == '_end_' or len(decoded_sentence) >= max_len_title:\n",
    "                stop_condition = True\n",
    "                break\n",
    "            decoded_sentence.append(pred_word_str)\n",
    "\n",
    "            # update the decoder for the next word\n",
    "            body_encoding = st\n",
    "            state_value = np.array(pred_idx).reshape(1, 1)\n",
    "\n",
    "        return original_body_encoding, ' '.join(decoded_sentence)\n",
    "\n",
    "\n",
    "    def print_example(self,\n",
    "                      i,\n",
    "                      body_text,\n",
    "                      title_text,\n",
    "                      threshold):\n",
    "        \"\"\"\n",
    "        Prints an example of the model's prediction for manual inspection.\n",
    "        \"\"\"\n",
    "        if i:\n",
    "            print('\\n\\n==============================================')\n",
    "            print(f'============== Example # {i} =================\\n')\n",
    "\n",
    "\n",
    "        print(f\"Context:\\n {body_text} \\n\")\n",
    "\n",
    "        if title_text:\n",
    "            print(f\"Original Utterance:\\n {title_text}\")\n",
    "\n",
    "        emb, gen_title = self.generate_issue_title(body_text)\n",
    "        print(f\"\\n****** Machine Generated Utterance (Prediction) ******:\\n {gen_title}\")\n",
    "\n",
    "        if self.nn:\n",
    "            # return neighbors and distances\n",
    "            n, d = self.nn.get_nns_by_vector(emb.flatten(), n=4,\n",
    "                                             include_distances=True)\n",
    "            neighbors = n[1:]\n",
    "            dist = d[1:]\n",
    "\n",
    "            if min(dist) <= threshold:\n",
    "                cols = ['issue_title', 'body']\n",
    "                dfcopy = self.rec_df.iloc[neighbors][cols].copy(deep=True)\n",
    "                dfcopy['dist'] = dist\n",
    "                similar_issues_df = dfcopy.query(f'dist <= {threshold}')\n",
    "\n",
    "                print(\"\\n**** Similar Issues (using encoder embedding) ****:\\n\")\n",
    "                display(similar_issues_df)\n",
    "\n",
    "\n",
    "    def demo_model_predictions(self,\n",
    "                               n,\n",
    "                               issue_df,\n",
    "                               threshold=1):\n",
    "        \"\"\"\n",
    "        Pick n random Issues and display predictions.\n",
    "        Input:\n",
    "        ------\n",
    "        n : int\n",
    "            Number of issues to display from issue_df\n",
    "        issue_df : pandas DataFrame\n",
    "            DataFrame that contains two columns: `body` and `issue_title`.\n",
    "        threshold : float\n",
    "            distance threshold for recommendation of similar issues.\n",
    "        Returns:\n",
    "        --------\n",
    "        None\n",
    "            Prints the original issue body and the model's prediction.\n",
    "        \"\"\"\n",
    "        # Extract body and title from DF\n",
    "        body_text = issue_df.Context.tolist()\n",
    "        title_text = issue_df.Utterance.tolist()\n",
    "\n",
    "        demo_list = np.random.randint(low=1, high=len(body_text), size=n)\n",
    "        for i in demo_list:\n",
    "            self.print_example(i,\n",
    "                               body_text=body_text[i],\n",
    "                               title_text=title_text[i],\n",
    "                               threshold=threshold)\n",
    "\n",
    "    def prepare_recommender(self, vectorized_array, original_df):\n",
    "        \"\"\"\n",
    "        Use the annoy library to build recommender\n",
    "        Parameters\n",
    "        ----------\n",
    "        vectorized_array : List[List[int]]\n",
    "            This is the list of list of integers that represents your corpus\n",
    "            that is fed into the seq2seq model for training.\n",
    "        original_df : pandas.DataFrame\n",
    "            This is the original dataframe that has the columns\n",
    "            ['issue_url', 'issue_title', 'body']\n",
    "        Returns\n",
    "        -------\n",
    "        annoy.AnnoyIndex  object (see https://github.com/spotify/annoy)\n",
    "        \"\"\"\n",
    "        self.rec_df = original_df\n",
    "        emb = self.encoder_model.predict(x=vectorized_array,\n",
    "                                         batch_size=vectorized_array.shape[0]//200)\n",
    "\n",
    "        f = emb.shape[1]\n",
    "        self.nn = AnnoyIndex(f)\n",
    "        logging.warning('Adding embeddings')\n",
    "        for i in tqdm(range(len(emb))):\n",
    "            self.nn.add_item(i, emb[i])\n",
    "        logging.warning('Building trees for similarity lookup.')\n",
    "        self.nn.build(50)\n",
    "        return self.nn\n",
    "\n",
    "    def set_recsys_data(self, original_df):\n",
    "        self.rec_df = original_df\n",
    "\n",
    "    def set_recsys_annoyobj(self, annoyobj):\n",
    "        self.nn = annoyobj\n",
    "\n",
    "    def evaluate_model(self, holdout_bodies, holdout_titles):\n",
    "        \"\"\"\n",
    "        Method for calculating BLEU Score.\n",
    "        Parameters\n",
    "        ----------\n",
    "        holdout_bodies : List[str]\n",
    "            These are the issue bodies that we want to summarize\n",
    "        holdout_titles : List[str]\n",
    "            This is the ground truth we are trying to predict --> issue titles\n",
    "        Returns\n",
    "        -------\n",
    "        bleu : float\n",
    "            The BLEU Score\n",
    "        \"\"\"\n",
    "        actual, predicted = list(), list()\n",
    "        assert len(holdout_bodies) == len(holdout_titles)\n",
    "        num_examples = len(holdout_bodies)\n",
    "\n",
    "        logging.warning('Generating predictions.')\n",
    "        # step over the whole set TODO: parallelize this\n",
    "        for i in tqdm_notebook(range(num_examples)):\n",
    "            _, yhat = self.generate_issue_title(holdout_bodies[i])\n",
    "\n",
    "            actual.append(self.pp_title.process_text([holdout_titles[i]])[0])\n",
    "            predicted.append(self.pp_title.process_text([yhat])[0])\n",
    "        # calculate BLEU score\n",
    "        logging.warning('Calculating BLEU.')\n",
    "        \n",
    "        #must be careful with nltk api for corpus_bleu!, \n",
    "        # expects List[List[List[str]]] for ground truth, using List[List[str]] will give you\n",
    "        # erroneous results.\n",
    "        bleu = corpus_bleu([[a] for a in actual], predicted)\n",
    "        return bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_encoder_model(model):\n",
    "    \"\"\"\n",
    "    Extract the encoder from the original Sequence to Sequence Model.\n",
    "    Returns a keras model object that has one input (body of issue) and one\n",
    "    output (encoding of issue, which is the last hidden state).\n",
    "    Input:\n",
    "    -----\n",
    "    model: keras model object\n",
    "    Returns:\n",
    "    -----\n",
    "    keras model object\n",
    "    \"\"\"\n",
    "    encoder_model = model.get_layer('Encoder-Model')\n",
    "    return encoder_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_decoder_model(model):\n",
    "    \"\"\"\n",
    "    Extract the decoder from the original model.\n",
    "    Inputs:\n",
    "    ------\n",
    "    model: keras model object\n",
    "    Returns:\n",
    "    -------\n",
    "    A Keras model object with the following inputs and outputs:\n",
    "    Inputs of Keras Model That Is Returned:\n",
    "    1: the embedding index for the last predicted word or the <Start> indicator\n",
    "    2: the last hidden state, or in the case of the first word the hidden state from the encoder\n",
    "    Outputs of Keras Model That Is Returned:\n",
    "    1.  Prediction (class probabilities) for the next word\n",
    "    2.  The hidden state of the decoder, to be fed back into the decoder at the next time step\n",
    "    Implementation Notes:\n",
    "    ----------------------\n",
    "    Must extract relevant layers and reconstruct part of the computation graph\n",
    "    to allow for different inputs as we are not going to use teacher forcing at\n",
    "    inference time.\n",
    "    \"\"\"\n",
    "    # the latent dimension is the same throughout the architecture so we are going to\n",
    "    # cheat and grab the latent dimension of the embedding because that is the same as what is\n",
    "    # output from the decoder\n",
    "    latent_dim = model.get_layer('Decoder-Word-Embedding').output_shape[-1]\n",
    "\n",
    "    # Reconstruct the input into the decoder\n",
    "    decoder_inputs = model.get_layer('Decoder-Input').input\n",
    "    dec_emb = model.get_layer('Decoder-Word-Embedding')(decoder_inputs)\n",
    "    dec_bn = model.get_layer('Decoder-Batchnorm-1')(dec_emb)\n",
    "\n",
    "    # Instead of setting the intial state from the encoder and forgetting about it, during inference\n",
    "    # we are not doing teacher forcing, so we will have to have a feedback loop from predictions back into\n",
    "    # the GRU, thus we define this input layer for the state so we can add this capability\n",
    "    gru_inference_state_input = Input(shape=(latent_dim,), name='hidden_state_input')\n",
    "\n",
    "    # we need to reuse the weights that is why we are getting this\n",
    "    # If you inspect the decoder GRU that we created for training, it will take as input\n",
    "    # 2 tensors -> (1) is the embedding layer output for the teacher forcing\n",
    "    #                  (which will now be the last step's prediction, and will be _start_ on the first time step)\n",
    "    #              (2) is the state, which we will initialize with the encoder on the first time step, but then\n",
    "    #                   grab the state after the first prediction and feed that back in again.\n",
    "    gru_out, gru_state_out = model.get_layer('Decoder-GRU')([dec_bn, gru_inference_state_input])\n",
    "\n",
    "    # Reconstruct dense layers\n",
    "    dec_bn2 = model.get_layer('Decoder-Batchnorm-2')(gru_out)\n",
    "    dense_out = model.get_layer('Final-Output-Dense')(dec_bn2)\n",
    "    decoder_model = Model([decoder_inputs, gru_inference_state_input],\n",
    "                          [dense_out, gru_state_out])\n",
    "    return decoder_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq2seq_inf = Seq2Seq_Inference(encoder_preprocessor=context_pp,\n",
    "                                 decoder_preprocessor=utterance_pp,\n",
    "                                 seq2seq_model=seq2seq_Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 1660 =================\n",
      "\n",
      "Context:\n",
      " i am not mixed up in anything hayseed what are you talking about \n",
      "\n",
      "Original Utterance:\n",
      " you just strike me as smart enough to be doing something else\n",
      "\n",
      "****** Machine Generated Utterance (Prediction) ******:\n",
      " you are not going to be a little girl\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 713 =================\n",
      "\n",
      "Context:\n",
      " watching the bitch trash my car does not count as a date \n",
      "\n",
      "Original Utterance:\n",
      " i got her under control she just acts crazed in public to keep up the image\n",
      "\n",
      "****** Machine Generated Utterance (Prediction) ******:\n",
      " do not know what you are going to be a little girl\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 1914 =================\n",
      "\n",
      "Context:\n",
      " the oh the spire how silly of me it is as plain as day mr merrick where did you learn to do this \n",
      "\n",
      "Original Utterance:\n",
      " i learned a long time ago\n",
      "\n",
      "****** Machine Generated Utterance (Prediction) ******:\n",
      " i am not going to be a little girl\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 251 =================\n",
      "\n",
      "Context:\n",
      " i am not really sure i had a shower and some sorbet \n",
      "\n",
      "Original Utterance:\n",
      " i think maybe you have got your dates mixed up\n",
      "\n",
      "****** Machine Generated Utterance (Prediction) ******:\n",
      " i am not going to be a little girl\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 416 =================\n",
      "\n",
      "Context:\n",
      " miss you too i have been seeing someone for a little while \n",
      "\n",
      "Original Utterance:\n",
      " oh great that is great\n",
      "\n",
      "****** Machine Generated Utterance (Prediction) ******:\n",
      " you are a little girl\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 103 =================\n",
      "\n",
      "Context:\n",
      " doolittle doolittle \n",
      "\n",
      "Original Utterance:\n",
      " yes yes\n",
      "\n",
      "****** Machine Generated Utterance (Prediction) ******:\n",
      " yeah yeah yeah\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 1507 =================\n",
      "\n",
      "Context:\n",
      " let me just run through a few things as a dealer you never gamble not anywhere  will need your picture \n",
      "\n",
      "Original Utterance:\n",
      " what for\n",
      "\n",
      "****** Machine Generated Utterance (Prediction) ******:\n",
      " i am not going to be a little girl\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 1901 =================\n",
      "\n",
      "Context:\n",
      " what is the plan \n",
      "\n",
      "Original Utterance:\n",
      " first of all we are going to the slow club to see dorothy vallens  will watch her for awhile i had like to hear her sing anyway and then also  will know she is there and not in her apartment\n",
      "\n",
      "****** Machine Generated Utterance (Prediction) ******:\n",
      " i am not going to be a little girl\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 749 =================\n",
      "\n",
      "Context:\n",
      " that bad \n",
      "\n",
      "Original Utterance:\n",
      " i feel like there is men here there is women here then there is men but hey what'd i expect\n",
      "\n",
      "****** Machine Generated Utterance (Prediction) ******:\n",
      " i am not going to be a little girl\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 1745 =================\n",
      "\n",
      "Context:\n",
      " my wife what is she got to do with you \n",
      "\n",
      "Original Utterance:\n",
      " no wonder he did not want to mention her name\n",
      "\n",
      "****** Machine Generated Utterance (Prediction) ******:\n",
      " i am not going to be a little girl\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 1406 =================\n",
      "\n",
      "Context:\n",
      " but i do not know maybe we are the normal ones y'know i mean what kind of people do well at this stuff \n",
      "\n",
      "Original Utterance:\n",
      " and i just liked you so much\n",
      "\n",
      "****** Machine Generated Utterance (Prediction) ******:\n",
      " i am sorry i am sorry\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 909 =================\n",
      "\n",
      "Context:\n",
      " need some help \n",
      "\n",
      "Original Utterance:\n",
      " you bet i do high tide comes right up to this road\n",
      "\n",
      "****** Machine Generated Utterance (Prediction) ******:\n",
      " i am not going to be a little girl\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 940 =================\n",
      "\n",
      "Context:\n",
      " you know what happened to the tower of babel do not you it fell down \n",
      "\n",
      "Original Utterance:\n",
      " you are sucking too much nitrogen in your mix\n",
      "\n",
      "****** Machine Generated Utterance (Prediction) ******:\n",
      " i am not going to be a little girl\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 768 =================\n",
      "\n",
      "Context:\n",
      " hmmm claire used to tell me i loved the event horizon more than i loved her i told her that was not true i just knew the event horizon better that is all \n",
      "\n",
      "Original Utterance:\n",
      " claire is your wife\n",
      "\n",
      "****** Machine Generated Utterance (Prediction) ******:\n",
      " i am not going to be a little girl\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 119 =================\n",
      "\n",
      "Context:\n",
      " you have a way of provoking his i have been watching you you seem impatient with your studies \n",
      "\n",
      "Original Utterance:\n",
      " to say the least i came here to expand my mind but honest inquiry seems strangled at every turn all we do is cling to the old knowledge instead of seeking the new\n",
      "\n",
      "****** Machine Generated Utterance (Prediction) ******:\n",
      " so what is the hell of course\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 1794 =================\n",
      "\n",
      "Context:\n",
      " but it is so good i mean it is so very good \n",
      "\n",
      "Original Utterance:\n",
      " thank you very much\n",
      "\n",
      "****** Machine Generated Utterance (Prediction) ******:\n",
      " you are not going to be a little girl\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 1196 =================\n",
      "\n",
      "Context:\n",
      " did you see jack \n",
      "\n",
      "Original Utterance:\n",
      " no in fact your wounds were cleaned and dressed before you arrived here\n",
      "\n",
      "****** Machine Generated Utterance (Prediction) ******:\n",
      " yeah yeah\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 1386 =================\n",
      "\n",
      "Context:\n",
      " i know i know \n",
      "\n",
      "Original Utterance:\n",
      " if you cannot deal with him on that basis you better get a new counselor\n",
      "\n",
      "****** Machine Generated Utterance (Prediction) ******:\n",
      " what is the hell of course\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 1630 =================\n",
      "\n",
      "Context:\n",
      " richard richard \n",
      "\n",
      "Original Utterance:\n",
      " what what\n",
      "\n",
      "****** Machine Generated Utterance (Prediction) ******:\n",
      " you are not going to be a little girl\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 1239 =================\n",
      "\n",
      "Context:\n",
      " both pilots \n",
      "\n",
      "Original Utterance:\n",
      " can you fly this airplane and land it\n",
      "\n",
      "****** Machine Generated Utterance (Prediction) ******:\n",
      " yeah yeah\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 409 =================\n",
      "\n",
      "Context:\n",
      " i cannot stand it \n",
      "\n",
      "Original Utterance:\n",
      " so you know what she does she is so grief stricken she runs to find solace in rudy levine's bed\n",
      "\n",
      "****** Machine Generated Utterance (Prediction) ******:\n",
      " what is the hell of course\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 1807 =================\n",
      "\n",
      "Context:\n",
      " what is the matter upset that i rubbed off on her \n",
      "\n",
      "Original Utterance:\n",
      " no impressed\n",
      "\n",
      "****** Machine Generated Utterance (Prediction) ******:\n",
      " i am not going to be a little girl\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 1476 =================\n",
      "\n",
      "Context:\n",
      " no no \n",
      "\n",
      "Original Utterance:\n",
      " how do you get through those winters well you are right juneau what is the highest peak\n",
      "\n",
      "****** Machine Generated Utterance (Prediction) ******:\n",
      " i am not going to be a little girl\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 1412 =================\n",
      "\n",
      "Context:\n",
      " yes yes \n",
      "\n",
      "Original Utterance:\n",
      " fine i will need a 500 retainer you can mail it\n",
      "\n",
      "****** Machine Generated Utterance (Prediction) ******:\n",
      " i am not going to be a little girl\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 401 =================\n",
      "\n",
      "Context:\n",
      " go ahead \n",
      "\n",
      "Original Utterance:\n",
      " you have great tits\n",
      "\n",
      "****** Machine Generated Utterance (Prediction) ******:\n",
      " you are not going to be a little girl\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 1338 =================\n",
      "\n",
      "Context:\n",
      " people get hurt \n",
      "\n",
      "Original Utterance:\n",
      " but it was an accident the house was under construction\n",
      "\n",
      "****** Machine Generated Utterance (Prediction) ******:\n",
      " you are not going to be a little girl\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 1822 =================\n",
      "\n",
      "Context:\n",
      " you do not need to feel bad about being sea sick you know \n",
      "\n",
      "Original Utterance:\n",
      " how can you help feeling bad when you are sea sick\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "****** Machine Generated Utterance (Prediction) ******:\n",
      " i am not going to be a little girl\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 1669 =================\n",
      "\n",
      "Context:\n",
      " this is the copy of swayzak's manning report that was released everybody on this job knows it is bullshit but we could never argue with the numbers they are all airtight \n",
      "\n",
      "Original Utterance:\n",
      " yeah airtight\n",
      "\n",
      "****** Machine Generated Utterance (Prediction) ******:\n",
      " i am not going to be a little girl\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 672 =================\n",
      "\n",
      "Context:\n",
      " if you are thinking of warning him do not put yourself out he cannot possibly escape \n",
      "\n",
      "Original Utterance:\n",
      " i stick my neck out for nobody\n",
      "\n",
      "****** Machine Generated Utterance (Prediction) ******:\n",
      " oh oh i am not going to be a little girl\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 111 =================\n",
      "\n",
      "Context:\n",
      " abort reed i put my company my name billions of dollars on the line and i will not let you make me look like a fool \n",
      "\n",
      "Original Utterance:\n",
      " victor if we could understand what happened to us\n",
      "\n",
      "****** Machine Generated Utterance (Prediction) ******:\n",
      " i am not going to be a little girl\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 619 =================\n",
      "\n",
      "Context:\n",
      " she was gonna get away \n",
      "\n",
      "Original Utterance:\n",
      " then let her get away i thought you were a pro you are supposed to be a fuckin' tracker\n",
      "\n",
      "****** Machine Generated Utterance (Prediction) ******:\n",
      " you are gonna be a little girl\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 24 =================\n",
      "\n",
      "Context:\n",
      " i think it is better if we find this man before he finds us again \n",
      "\n",
      "Original Utterance:\n",
      " i will see what i can do how do i get in touch with you\n",
      "\n",
      "****** Machine Generated Utterance (Prediction) ******:\n",
      " i am not going to be a little girl\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 1390 =================\n",
      "\n",
      "Context:\n",
      " there is a million places to hide around here \n",
      "\n",
      "Original Utterance:\n",
      " oh yeah they will never catch the guy\n",
      "\n",
      "****** Machine Generated Utterance (Prediction) ******:\n",
      " i am not going to be a little girl\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 280 =================\n",
      "\n",
      "Context:\n",
      " i kill someone famous \n",
      "\n",
      "Original Utterance:\n",
      " then do it asshole\n",
      "\n",
      "****** Machine Generated Utterance (Prediction) ******:\n",
      " then you are not going to be a little girl\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 627 =================\n",
      "\n",
      "Context:\n",
      " sir august merryweather i was looking for something relaxing say a tuscan hillside in june \n",
      "\n",
      "Original Utterance:\n",
      " normally we would be eager to oblige\n",
      "\n",
      "****** Machine Generated Utterance (Prediction) ******:\n",
      " what is the hell of course\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 965 =================\n",
      "\n",
      "Context:\n",
      " are you married \n",
      "\n",
      "Original Utterance:\n",
      " divorced divorced\n",
      "\n",
      "****** Machine Generated Utterance (Prediction) ******:\n",
      " i am not going to be a little girl\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 1882 =================\n",
      "\n",
      "Context:\n",
      " over the river \n",
      "\n",
      "Original Utterance:\n",
      " have you got transport\n",
      "\n",
      "****** Machine Generated Utterance (Prediction) ******:\n",
      " the hell are you talking about\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 84 =================\n",
      "\n",
      "Context:\n",
      " you can change it if you want to \n",
      "\n",
      "Original Utterance:\n",
      " i do not know why we have to watch tv\n",
      "\n",
      "****** Machine Generated Utterance (Prediction) ******:\n",
      " i am not going to be a little girl\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 792 =================\n",
      "\n",
      "Context:\n",
      " i have brought you some things i hope you will like mr merrick i hope you do not think it too forward \n",
      "\n",
      "Original Utterance:\n",
      " oh no\n",
      "\n",
      "****** Machine Generated Utterance (Prediction) ******:\n",
      " not a little girl\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 110 =================\n",
      "\n",
      "Context:\n",
      " oh boy a jacket \n",
      "\n",
      "Original Utterance:\n",
      " your mom made that all by herself\n",
      "\n",
      "****** Machine Generated Utterance (Prediction) ******:\n",
      " what what\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 531 =================\n",
      "\n",
      "Context:\n",
      " you will be a jehovah's witness i have a few awake magazines for you you do not have to keep her very long a few seconds is all i will need whatiya think \n",
      "\n",
      "Original Utterance:\n",
      " i do not know it sounds like a good daydream but actually doing it is too weird too dangerous\n",
      "\n",
      "****** Machine Generated Utterance (Prediction) ******:\n",
      " i do not know what i am not going to be a\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 1224 =================\n",
      "\n",
      "Context:\n",
      " that is some sick shit right there did she comb your ass hair for you too \n",
      "\n",
      "Original Utterance:\n",
      " if your mom so much as smells those tickets they are history and we get screwed outta seeing kiss for the third year in a row the third year\n",
      "\n",
      "****** Machine Generated Utterance (Prediction) ******:\n",
      " i am not going to be a little girl\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 1557 =================\n",
      "\n",
      "Context:\n",
      " how about fifteen hundred \n",
      "\n",
      "Original Utterance:\n",
      " how about five hundred\n",
      "\n",
      "****** Machine Generated Utterance (Prediction) ******:\n",
      " i am not going to be a little girl\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 791 =================\n",
      "\n",
      "Context:\n",
      " yeah yeah \n",
      "\n",
      "Original Utterance:\n",
      " did you send this is it a joke\n",
      "\n",
      "****** Machine Generated Utterance (Prediction) ******:\n",
      " i am not going to be a little girl\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 558 =================\n",
      "\n",
      "Context:\n",
      " i am sorry i really should not have \n",
      "\n",
      "Original Utterance:\n",
      " i am glad you called\n",
      "\n",
      "****** Machine Generated Utterance (Prediction) ******:\n",
      " i am not going to be a little girl\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 1587 =================\n",
      "\n",
      "Context:\n",
      " i understand \n",
      "\n",
      "Original Utterance:\n",
      " i just wanted to say au revoir and thank you for your help my friend\n",
      "\n",
      "****** Machine Generated Utterance (Prediction) ******:\n",
      " yes yes\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 849 =================\n",
      "\n",
      "Context:\n",
      " i think we should go wake them up just in case \n",
      "\n",
      "Original Utterance:\n",
      " give them a little while longer it is still early anyway\n",
      "\n",
      "****** Machine Generated Utterance (Prediction) ******:\n",
      " i am not going to be a little girl\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 481 =================\n",
      "\n",
      "Context:\n",
      " i had it on the island with me \n",
      "\n",
      "Original Utterance:\n",
      " must be a story there\n",
      "\n",
      "****** Machine Generated Utterance (Prediction) ******:\n",
      " i am not going to be a little girl\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 491 =================\n",
      "\n",
      "Context:\n",
      " that is the way i have always wanted it to be elaine \n",
      "\n",
      "Original Utterance:\n",
      " but it will not be not as long as you insist on living in the past\n",
      "\n",
      "****** Machine Generated Utterance (Prediction) ******:\n",
      " i am sorry i am sorry\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 1004 =================\n",
      "\n",
      "Context:\n",
      " they did not make it \n",
      "\n",
      "Original Utterance:\n",
      " nick my men and i will hold them here you will have to go and get help\n",
      "\n",
      "****** Machine Generated Utterance (Prediction) ******:\n",
      " i am not going to be a little girl\n"
     ]
    }
   ],
   "source": [
    "seq2seq_inf.demo_model_predictions(n=50, issue_df=testdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import re # For cleaning and replacing the texts of the corpus\n",
    "import time # To measure the training time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Context</th>\n",
       "      <th>Utterance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>well i thought we would start with pronunciati...</td>\n",
       "      <td>not the hacking and gagging and spitting part ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>not the hacking and gagging and spitting part ...</td>\n",
       "      <td>okay then how 'bout we try out some french cui...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>you are asking me out that is so cute what is ...</td>\n",
       "      <td>forget it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>no no it is my fault we did not have a proper ...</td>\n",
       "      <td>cameron cameron</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cameron cameron</td>\n",
       "      <td>the thing is cameron i am at the mercy of a pa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>the thing is cameron i am at the mercy of a pa...</td>\n",
       "      <td>seems like she could get a date easy enough</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>why why</td>\n",
       "      <td>unsolved mystery she used to be really popular...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>unsolved mystery she used to be really popular...</td>\n",
       "      <td>that is a shame</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>gosh if only we could find kat a boyfriend</td>\n",
       "      <td>let me see what i can do</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>that is because it is such a nice one</td>\n",
       "      <td>forget french</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>how is our little find the wench a date plan p...</td>\n",
       "      <td>well there is someone i think might be</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>there there</td>\n",
       "      <td>where where</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>you got something on your mind</td>\n",
       "      <td>i counted on you to help my cause you and that...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>you have my word as a gentleman</td>\n",
       "      <td>you are sweet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>sure have</td>\n",
       "      <td>i really really really wanna go but i cannot n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>i really really really wanna go but i cannot n...</td>\n",
       "      <td>i am workin' on it but she does not seem to be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>she is not a</td>\n",
       "      <td>lesbian no i found a picture of jared leto in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>lesbian no i found a picture of jared leto in ...</td>\n",
       "      <td>so that is the kind of guy she likes pretty ones</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>so that is the kind of guy she likes pretty ones</td>\n",
       "      <td>who knows all i have ever heard her say is tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>hi hi</td>\n",
       "      <td>looks like things worked out tonight huh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>you know chastity</td>\n",
       "      <td>i believe we share an art instructor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>have fun tonight</td>\n",
       "      <td>tons tons</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>i looked for you back at the party but you alw...</td>\n",
       "      <td>i was</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>i was</td>\n",
       "      <td>you never wanted to go out with 'me did you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>well no</td>\n",
       "      <td>then that is all you had to say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>then that is all you had to say</td>\n",
       "      <td>but but</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>but but</td>\n",
       "      <td>you always been this selfish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>do you listen to this crap</td>\n",
       "      <td>what crap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>i figured you would get to the good stuff even...</td>\n",
       "      <td>what good stuff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>what good stuff</td>\n",
       "      <td>the real you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19970</th>\n",
       "      <td>prepare a tear harness for the female</td>\n",
       "      <td>no i swear i do not know please</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19971</th>\n",
       "      <td>no i swear i do not know please</td>\n",
       "      <td>do you think i am a fool that the commander do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19972</th>\n",
       "      <td>wait what did you say</td>\n",
       "      <td>please do not hurt them it is not their fault ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19973</th>\n",
       "      <td>explain</td>\n",
       "      <td>gwen the show there is no choice do it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19974</th>\n",
       "      <td>our dimwitted friends do not understand the co...</td>\n",
       "      <td>we pretend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19975</th>\n",
       "      <td>we pretend</td>\n",
       "      <td>simpler simpler</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19976</th>\n",
       "      <td>simpler simpler</td>\n",
       "      <td>we we lie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19977</th>\n",
       "      <td>we we lie</td>\n",
       "      <td>yes you understand that do not you mathesar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19978</th>\n",
       "      <td>accelerate to mark tommy</td>\n",
       "      <td>this is embarrassing really i shall not tell t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19979</th>\n",
       "      <td>right if this is about the thing tomorrow you ...</td>\n",
       "      <td>i certainly but</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19980</th>\n",
       "      <td>i certainly but</td>\n",
       "      <td>catch me later okay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19981</th>\n",
       "      <td>commander welcome to the protector ii would yo...</td>\n",
       "      <td>mind if we skip that i have to get back pretty...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19982</th>\n",
       "      <td>mind if we skip that i have to get back pretty...</td>\n",
       "      <td>as you wish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19983</th>\n",
       "      <td>commander where are you going</td>\n",
       "      <td>home home</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19984</th>\n",
       "      <td>home home</td>\n",
       "      <td>you you mean earth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19985</th>\n",
       "      <td>you you mean earth</td>\n",
       "      <td>yeah earth time to get back to earth kids</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19986</th>\n",
       "      <td>but commander the negotiation you you you fire...</td>\n",
       "      <td>right long live what is your planet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19987</th>\n",
       "      <td>right long live what is your planet</td>\n",
       "      <td>theramin theramin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19988</th>\n",
       "      <td>but what if sarris survives</td>\n",
       "      <td>oh i do not think so i gave him both barrels</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19989</th>\n",
       "      <td>oh i do not think so i gave him both barrels</td>\n",
       "      <td>he has a very powerful ship perhaps you would ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19990</th>\n",
       "      <td>an interstellar vox</td>\n",
       "      <td>thanks thanks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19991</th>\n",
       "      <td>how can we thank you commander you you have sa...</td>\n",
       "      <td>it was a lot of fun you kids are great</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19992</th>\n",
       "      <td>weapons storage</td>\n",
       "      <td>it is perfectly safe i promise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19993</th>\n",
       "      <td>it is perfectly safe i promise</td>\n",
       "      <td>maintenance facility</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19994</th>\n",
       "      <td>to our brave guests few in this universe have ...</td>\n",
       "      <td>wherever a distress signal sounds among the st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19995</th>\n",
       "      <td>mathesar has sarris seen the historical records</td>\n",
       "      <td>no thank god he has not</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19996</th>\n",
       "      <td>no thank god he has not</td>\n",
       "      <td>then how did he find out about the device</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19997</th>\n",
       "      <td>then how did he find out about the device</td>\n",
       "      <td>our former commander was not strong</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19998</th>\n",
       "      <td>our former commander was not strong</td>\n",
       "      <td>former commander</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19999</th>\n",
       "      <td>former commander</td>\n",
       "      <td>i am sorry you deserve to be shown</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Context  \\\n",
       "0      well i thought we would start with pronunciati...   \n",
       "1      not the hacking and gagging and spitting part ...   \n",
       "2      you are asking me out that is so cute what is ...   \n",
       "3      no no it is my fault we did not have a proper ...   \n",
       "4                                        cameron cameron   \n",
       "5      the thing is cameron i am at the mercy of a pa...   \n",
       "6                                                why why   \n",
       "7      unsolved mystery she used to be really popular...   \n",
       "8             gosh if only we could find kat a boyfriend   \n",
       "9                  that is because it is such a nice one   \n",
       "10     how is our little find the wench a date plan p...   \n",
       "11                                           there there   \n",
       "12                        you got something on your mind   \n",
       "13                       you have my word as a gentleman   \n",
       "14                                             sure have   \n",
       "15     i really really really wanna go but i cannot n...   \n",
       "16                                          she is not a   \n",
       "17     lesbian no i found a picture of jared leto in ...   \n",
       "18      so that is the kind of guy she likes pretty ones   \n",
       "19                                                 hi hi   \n",
       "20                                     you know chastity   \n",
       "21                                      have fun tonight   \n",
       "22     i looked for you back at the party but you alw...   \n",
       "23                                                 i was   \n",
       "24                                               well no   \n",
       "25                       then that is all you had to say   \n",
       "26                                               but but   \n",
       "27                            do you listen to this crap   \n",
       "28     i figured you would get to the good stuff even...   \n",
       "29                                       what good stuff   \n",
       "...                                                  ...   \n",
       "19970              prepare a tear harness for the female   \n",
       "19971                    no i swear i do not know please   \n",
       "19972                              wait what did you say   \n",
       "19973                                            explain   \n",
       "19974  our dimwitted friends do not understand the co...   \n",
       "19975                                         we pretend   \n",
       "19976                                    simpler simpler   \n",
       "19977                                          we we lie   \n",
       "19978                           accelerate to mark tommy   \n",
       "19979  right if this is about the thing tomorrow you ...   \n",
       "19980                                    i certainly but   \n",
       "19981  commander welcome to the protector ii would yo...   \n",
       "19982  mind if we skip that i have to get back pretty...   \n",
       "19983                      commander where are you going   \n",
       "19984                                          home home   \n",
       "19985                                 you you mean earth   \n",
       "19986  but commander the negotiation you you you fire...   \n",
       "19987                right long live what is your planet   \n",
       "19988                        but what if sarris survives   \n",
       "19989       oh i do not think so i gave him both barrels   \n",
       "19990                                an interstellar vox   \n",
       "19991  how can we thank you commander you you have sa...   \n",
       "19992                                    weapons storage   \n",
       "19993                     it is perfectly safe i promise   \n",
       "19994  to our brave guests few in this universe have ...   \n",
       "19995    mathesar has sarris seen the historical records   \n",
       "19996                            no thank god he has not   \n",
       "19997          then how did he find out about the device   \n",
       "19998                our former commander was not strong   \n",
       "19999                                   former commander   \n",
       "\n",
       "                                               Utterance  \n",
       "0      not the hacking and gagging and spitting part ...  \n",
       "1      okay then how 'bout we try out some french cui...  \n",
       "2                                              forget it  \n",
       "3                                        cameron cameron  \n",
       "4      the thing is cameron i am at the mercy of a pa...  \n",
       "5            seems like she could get a date easy enough  \n",
       "6      unsolved mystery she used to be really popular...  \n",
       "7                                        that is a shame  \n",
       "8                               let me see what i can do  \n",
       "9                                          forget french  \n",
       "10                well there is someone i think might be  \n",
       "11                                           where where  \n",
       "12     i counted on you to help my cause you and that...  \n",
       "13                                         you are sweet  \n",
       "14     i really really really wanna go but i cannot n...  \n",
       "15     i am workin' on it but she does not seem to be...  \n",
       "16     lesbian no i found a picture of jared leto in ...  \n",
       "17      so that is the kind of guy she likes pretty ones  \n",
       "18     who knows all i have ever heard her say is tha...  \n",
       "19              looks like things worked out tonight huh  \n",
       "20                  i believe we share an art instructor  \n",
       "21                                             tons tons  \n",
       "22                                                 i was  \n",
       "23           you never wanted to go out with 'me did you  \n",
       "24                       then that is all you had to say  \n",
       "25                                               but but  \n",
       "26                          you always been this selfish  \n",
       "27                                             what crap  \n",
       "28                                       what good stuff  \n",
       "29                                          the real you  \n",
       "...                                                  ...  \n",
       "19970                    no i swear i do not know please  \n",
       "19971  do you think i am a fool that the commander do...  \n",
       "19972  please do not hurt them it is not their fault ...  \n",
       "19973             gwen the show there is no choice do it  \n",
       "19974                                         we pretend  \n",
       "19975                                    simpler simpler  \n",
       "19976                                          we we lie  \n",
       "19977        yes you understand that do not you mathesar  \n",
       "19978  this is embarrassing really i shall not tell t...  \n",
       "19979                                    i certainly but  \n",
       "19980                                catch me later okay  \n",
       "19981  mind if we skip that i have to get back pretty...  \n",
       "19982                                        as you wish  \n",
       "19983                                          home home  \n",
       "19984                                 you you mean earth  \n",
       "19985          yeah earth time to get back to earth kids  \n",
       "19986                right long live what is your planet  \n",
       "19987                                  theramin theramin  \n",
       "19988       oh i do not think so i gave him both barrels  \n",
       "19989  he has a very powerful ship perhaps you would ...  \n",
       "19990                                      thanks thanks  \n",
       "19991             it was a lot of fun you kids are great  \n",
       "19992                     it is perfectly safe i promise  \n",
       "19993                               maintenance facility  \n",
       "19994  wherever a distress signal sounds among the st...  \n",
       "19995                            no thank god he has not  \n",
       "19996          then how did he find out about the device  \n",
       "19997                our former commander was not strong  \n",
       "19998                                   former commander  \n",
       "19999                 i am sorry you deserve to be shown  \n",
       "\n",
       "[20000 rows x 2 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_questions = data.Context.tolist()\n",
    "clean_answers = data.Utterance.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_questions = []\n",
    "short_answers = []\n",
    "i = 0\n",
    "for question in clean_questions:\n",
    "    if 2 <= len(question.split()) <= 25: # Taking a range from 2 to 25 for the training of dataset (since both short will be deficient and long data will be overwhelming for chatbot to learn)\n",
    "        short_questions.append(question)\n",
    "        short_answers.append(clean_answers[i])\n",
    "    i += 1 # Incrementing the value of i so that it iterates each and every conversation\n",
    "# In the above step, we filter from the questions and include the subsequent answers\n",
    "clean_questions = []\n",
    "clean_answers = []\n",
    "i = 0\n",
    "for answer in short_answers:\n",
    "    if 2 <= len(answer.split()) <= 25:\n",
    "        clean_answers.append(answer)\n",
    "        clean_questions.append(short_questions[i])\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2count = {} \n",
    "for question in clean_questions:\n",
    "    for word in question.split():\n",
    "        if word not in word2count:\n",
    "            word2count[word] = 1 # If the word is not present in dictionary, initiate its count\n",
    "        else:\n",
    "            word2count[word] += 1 # If the word is already included in the dictionary, increment the count by 1\n",
    "for answer in clean_answers:\n",
    "    for word in answer.split():\n",
    "        if word not in word2count:\n",
    "            word2count[word] = 1\n",
    "        else:\n",
    "            word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_questions = 15 # Creating a threshold so as to include only the words with occurences greater than or equal to 15\n",
    "questionswords2int = {}\n",
    "word_number = 0 \n",
    "for word, count in word2count.items():\n",
    "    if count >= threshold_questions:\n",
    "        questionswords2int[word] = word_number\n",
    "        word_number += 1\n",
    "threshold_answers = 15 # Instead of using two different variables for threshold, we can use just a single variable\n",
    "answerswords2int = {}\n",
    "word_number = 0\n",
    "for word, count in word2count.items():\n",
    "    if count >= threshold_answers:\n",
    "        answerswords2int[word] = word_number\n",
    "        word_number += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = ['<PAD>', '<EOS>', '<OUT>', '<SOS>']\n",
    "for token in tokens:\n",
    "    questionswords2int[token] = len(questionswords2int) + 1 # Since upper-bound value is excluded, so +1 to include the upper_bound value\n",
    "for token in tokens:\n",
    "    answerswords2int[token] = len(answerswords2int) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "answersints2word = {w_i: w for w, w_i in answerswords2int.items()}\n",
    " \n",
    "# Step 11: Adding <EOS> to end of every answer in the dictionary\n",
    "for i in range(len(clean_answers)):\n",
    "    clean_answers[i] += ' <EOS>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_into_int = []\n",
    "for question in clean_questions:\n",
    "    ints = []\n",
    "    for word in question.split(): # Traversing each and every word present in the clean_questions list\n",
    "        if word not in questionswords2int:\n",
    "            ints.append(questionswords2int['<OUT>'])\n",
    "        else:\n",
    "            ints.append(questionswords2int[word])\n",
    "    questions_into_int.append(ints)\n",
    "answers_into_int = []\n",
    "for answer in clean_answers:\n",
    "    ints = []\n",
    "    for word in answer.split():\n",
    "        if word not in answerswords2int:\n",
    "            ints.append(answerswords2int['<OUT>'])\n",
    "        else:\n",
    "            ints.append(answerswords2int[word])\n",
    "    answers_into_int.append(ints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_clean_questions = []\n",
    "sorted_clean_answers = []\n",
    "for length in range(1, 25 + 1): # Even though the range is from 1 to 25, but the questions with minimum words will have atleast 2 words due to previous constraints\n",
    "    for i in enumerate(questions_into_int):\n",
    "        if len(i[1]) == length:\n",
    "            sorted_clean_questions.append(questions_into_int[i[0]])\n",
    "            sorted_clean_answers.append(answers_into_int[i[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_inputs(): # Function for defining tensorflow placeholders\n",
    "    inputs = tf.placeholder(tf.int32, [None, None], name = 'input') \n",
    "    targets = tf.placeholder(tf.int32, [None, None], name = 'target')\n",
    "    lr = tf.placeholder(tf.float32, name = 'learning_rate')\n",
    "    keep_prob = tf.placeholder(tf.float32, name = 'keep_prob')\n",
    "    return inputs, targets, lr, keep_prob\n",
    "def preprocess_targets(targets, word2int, batch_size): # The targets must be in batches & each of the answers of the target must start with SOS token\n",
    "    left_side = tf.fill([batch_size, 1], word2int['<SOS>']) # Making a vector of <SOS> token, size being batch_size*1\n",
    "    right_side = tf.strided_slice(targets, [0,0], [batch_size, -1], [1,1]) # Making a vector targets without the last column and slided element by element\n",
    "    preprocessed_targets = tf.concat([left_side, right_side], 1) # Concatenating both the vectors horizontally\n",
    "    return preprocessed_targets\n",
    "def encoder_rnn(rnn_inputs, rnn_size, num_layers, keep_prob, sequence_length):\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(rnn_size) \n",
    "    lstm_dropout = tf.contrib.rnn.DropoutWrapper(lstm, input_keep_prob = keep_prob) # Applying dropout to LSTM so as to drop specific no of neurons during training of chatbot so as train it efficiently\n",
    "    encoder_cell = tf.contrib.rnn.MultiRNNCell([lstm_dropout] * num_layers)\n",
    "    _, encoder_state = tf.nn.bidirectional_dynamic_rnn(cell_fw = encoder_cell, # The underscore is initially used because bidirectional rnn gives 2 output but we are interested in just the second one\n",
    "                                                                    cell_bw = encoder_cell,\n",
    "                                                                    sequence_length = sequence_length,\n",
    "                                                                    inputs = rnn_inputs,\n",
    "                                                                    dtype = tf.float32)\n",
    "    return encoder_state\n",
    "def decode_training_set(encoder_state, decoder_cell, decoder_embedded_input, sequence_length, decoding_scope, output_function, keep_prob, batch_size):\n",
    "    attention_states = tf.zeros([batch_size, 1, decoder_cell.output_size]) # A three dimensional matrix of zeros\n",
    "    attention_keys, attention_values, attention_score_function, attention_construct_function = tf.contrib.seq2seq.prepare_attention(attention_states, attention_option = \"bahdanau\", num_units = decoder_cell.output_size)\n",
    "    training_decoder_function = tf.contrib.seq2seq.attention_decoder_fn_train(encoder_state[0],\n",
    "                                                                              attention_keys,\n",
    "                                                                              attention_values,\n",
    "                                                                              attention_score_function,\n",
    "                                                                              attention_construct_function,\n",
    "                                                                              name = \"attn_dec_train\")\n",
    "    decoder_output, _, _ = tf.contrib.seq2seq.dynamic_rnn_decoder(decoder_cell, # Again underscores used due to the similar reasons as mentioned in the above function\n",
    "                                                                  training_decoder_function,\n",
    "                                                                  decoder_embedded_input,\n",
    "                                                                  sequence_length,\n",
    "                                                                  scope = decoding_scope)\n",
    "    decoder_output_dropout = tf.nn.dropout(decoder_output, keep_prob)\n",
    "    return output_function(decoder_output_dropout)\n",
    "def decode_test_set(encoder_state, decoder_cell, decoder_embeddings_matrix, sos_id, eos_id, maximum_length, num_words, decoding_scope, output_function, keep_prob, batch_size):\n",
    "    attention_states = tf.zeros([batch_size, 1, decoder_cell.output_size]) # A three dimensional matrix of zeros\n",
    "    attention_keys, attention_values, attention_score_function, attention_construct_function = tf.contrib.seq2seq.prepare_attention(attention_states, attention_option = \"bahdanau\", num_units = decoder_cell.output_size)\n",
    "    test_decoder_function = tf.contrib.seq2seq.attention_decoder_fn_inference(output_function,\n",
    "                                                                              encoder_state[0],\n",
    "                                                                              attention_keys,\n",
    "                                                                              attention_values,\n",
    "                                                                              attention_score_function,\n",
    "                                                                              attention_construct_function,\n",
    "                                                                              decoder_embeddings_matrix,\n",
    "                                                                              sos_id,\n",
    "                                                                              eos_id,\n",
    "                                                                              maximum_length,\n",
    "                                                                              num_words,\n",
    "                                                                              name = \"attn_dec_inf\")\n",
    "    test_predictions, _, _ = tf.contrib.seq2seq.dynamic_rnn_decoder(decoder_cell,\n",
    "                                                                    test_decoder_function,\n",
    "                                                                    scope = decoding_scope)\n",
    "    return test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_rnn(decoder_embedded_input, decoder_embeddings_matrix, encoder_state, num_words, sequence_length, rnn_size, num_layers, word2int, keep_prob, batch_size):\n",
    "    with tf.variable_scope(\"decoding\") as decoding_scope:\n",
    "        lstm = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
    "        lstm_dropout = tf.contrib.rnn.DropoutWrapper(lstm, input_keep_prob = keep_prob)\n",
    "        decoder_cell = tf.contrib.rnn.MultiRNNCell([lstm_dropout] * num_layers)\n",
    "        weights = tf.truncated_normal_initializer(stddev = 0.1) # Getting weights to be a truncated normal distribution\n",
    "        biases = tf.zeros_initializer() # Initializing biases value to be 0\n",
    "        output_function = lambda x: tf.contrib.layers.fully_connected(x,\n",
    "                                                                      num_words,\n",
    "                                                                      None,\n",
    "                                                                      scope = decoding_scope,\n",
    "                                                                      weights_initializer = weights,\n",
    "                                                                      biases_initializer = biases)\n",
    "        training_predictions = decode_training_set(encoder_state,\n",
    "                                                   decoder_cell,\n",
    "                                                   decoder_embedded_input,\n",
    "                                                   sequence_length,\n",
    "                                                   decoding_scope,\n",
    "                                                   output_function,\n",
    "                                                   keep_prob,\n",
    "                                                   batch_size)\n",
    "        decoding_scope.reuse_variables() # So as to use the same variables as used in the above function\n",
    "        test_predictions = decode_test_set(encoder_state,\n",
    "                                           decoder_cell,\n",
    "                                           decoder_embeddings_matrix,\n",
    "                                           word2int['<SOS>'],\n",
    "                                           word2int['<EOS>'],\n",
    "                                           sequence_length - 1,\n",
    "                                           num_words,\n",
    "                                           decoding_scope,\n",
    "                                           output_function,\n",
    "                                           keep_prob,\n",
    "                                           batch_size)\n",
    "    return training_predictions, test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2seq_model(inputs, targets, keep_prob, batch_size, sequence_length, answers_num_words, questions_num_words, encoder_embedding_size, decoder_embedding_size, rnn_size, num_layers, questionswords2int):\n",
    "    encoder_embedded_input = tf.contrib.layers.embed_sequence(inputs,\n",
    "                                                              answers_num_words + 1,\n",
    "                                                              encoder_embedding_size,\n",
    "                                                              initializer = tf.random_uniform_initializer(0, 1))\n",
    "    encoder_state = encoder_rnn(encoder_embedded_input, rnn_size, num_layers, keep_prob, sequence_length)\n",
    "    preprocessed_targets = preprocess_targets(targets, questionswords2int, batch_size)\n",
    "    decoder_embeddings_matrix = tf.Variable(tf.random_uniform([questions_num_words + 1, decoder_embedding_size], 0, 1))\n",
    "    decoder_embedded_input = tf.nn.embedding_lookup(decoder_embeddings_matrix, preprocessed_targets)\n",
    "    training_predictions, test_predictions = decoder_rnn(decoder_embedded_input,\n",
    "                                                         decoder_embeddings_matrix,\n",
    "                                                         encoder_state,\n",
    "                                                         questions_num_words,\n",
    "                                                         sequence_length,\n",
    "                                                         rnn_size,\n",
    "                                                         num_layers,\n",
    "                                                         questionswords2int,\n",
    "                                                         keep_prob,\n",
    "                                                         batch_size)\n",
    "    return training_predictions, test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "batch_size = 32\n",
    "rnn_size = 32\n",
    "num_layers = 2\n",
    "encoding_embedding_size = 32\n",
    "decoding_embedding_size = 32\n",
    "learning_rate = 0.001\n",
    "learning_rate_decay = 0.9\n",
    "min_learning_rate = 0.0001\n",
    "keep_probability = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "session = tf.InteractiveSession()\n",
    " \n",
    "# Step 3: Loading the model inputs\n",
    "inputs, targets, lr, keep_prob = model_inputs()\n",
    " \n",
    "# Step 4: Setting the sequence length (will be max of the data length in dataset)\n",
    "sequence_length = tf.placeholder_with_default(25, None, name = 'sequence_length')\n",
    " \n",
    "# Step 5: Getting the shape of the inputs tensor\n",
    "input_shape = tf.shape(inputs)\n",
    " \n",
    "# Step 6: Getting the training and test predictions\n",
    "training_predictions, test_predictions = seq2seq_model(tf.reverse(inputs, [-1]),\n",
    "                                                       targets,\n",
    "                                                       keep_prob,\n",
    "                                                       batch_size,\n",
    "                                                       sequence_length,\n",
    "                                                       len(answerswords2int),\n",
    "                                                       len(questionswords2int),\n",
    "                                                       encoding_embedding_size,\n",
    "                                                       decoding_embedding_size,\n",
    "                                                       rnn_size,\n",
    "                                                       num_layers,\n",
    "                                                       questionswords2int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"optimization\"):\n",
    "    loss_error = tf.contrib.seq2seq.sequence_loss(training_predictions,\n",
    "                                                  targets,\n",
    "                                                  tf.ones([input_shape[0], sequence_length]))\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    gradients = optimizer.compute_gradients(loss_error)\n",
    "    clipped_gradients = [(tf.clip_by_value(grad_tensor, -5., 5.), grad_variable) for grad_tensor, grad_variable in gradients if grad_tensor is not None] # Clipping the gradient from -5 to 5\n",
    "    optimizer_gradient_clipping = optimizer.apply_gradients(clipped_gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_padding(batch_of_sequences, word2int):\n",
    "    max_sequence_length = max([len(sequence) for sequence in batch_of_sequences]) # Max length data used so as to get a proper padding of data so that each data is of same and equal length\n",
    "    return [sequence + [word2int['<PAD>']] * (max_sequence_length - len(sequence)) for sequence in batch_of_sequences]\n",
    " \n",
    "# Step 9: Splitting the data into batches of questions and answers\n",
    "def split_into_batches(questions, answers, batch_size):\n",
    "    for batch_index in range(0, len(questions) // batch_size):\n",
    "        start_index = batch_index * batch_size\n",
    "        questions_in_batch = questions[start_index : start_index + batch_size]\n",
    "        answers_in_batch = answers[start_index : start_index + batch_size]\n",
    "        padded_questions_in_batch = np.array(apply_padding(questions_in_batch, questionswords2int))\n",
    "        padded_answers_in_batch = np.array(apply_padding(answers_in_batch, answerswords2int))\n",
    "        yield padded_questions_in_batch, padded_answers_in_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_validation_split = int(len(sorted_clean_questions) * 0.15)\n",
    "training_questions = sorted_clean_questions[training_validation_split:]\n",
    "training_answers = sorted_clean_answers[training_validation_split:]\n",
    "validation_questions = sorted_clean_questions[:training_validation_split]\n",
    "validation_answers = sorted_clean_answers[:training_validation_split]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_index_check_training_loss = 100 # Checking training loss every 100 epochs\n",
    "batch_index_check_validation_loss = ((len(training_questions)) // batch_size // 2) - 1 # Checking validation loss every half of batch size\n",
    "total_training_loss_error = 0\n",
    "list_validation_loss_error = []\n",
    "early_stopping_check = 0 # This value gets incremented by 1 if the loss error doesn't increase with each iteration\n",
    "early_stopping_stop = 100 # Used to check if the early_stopping_check becomes its equal, if yes, then we break the process\n",
    "checkpoint = \"chatbot_weights.ckpt\"\n",
    "session.run(tf.global_variables_initializer())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   1/10, Batch:    0/472, Training Loss Error:  0.072, Training Time on 100 Batches: 51 seconds\n",
      "Epoch:   1/10, Batch:  100/472, Training Loss Error:  3.930, Training Time on 100 Batches: 16 seconds\n",
      "Epoch:   1/10, Batch:  200/472, Training Loss Error:  2.455, Training Time on 100 Batches: 18 seconds\n",
      "Validation Loss Error:  2.052, Batch Validation Time: 4 seconds\n",
      "I am getting better as time goes by.\n",
      "Epoch:   1/10, Batch:  300/472, Training Loss Error:  2.198, Training Time on 100 Batches: 21 seconds\n",
      "Epoch:   1/10, Batch:  400/472, Training Loss Error:  2.144, Training Time on 100 Batches: 22 seconds\n",
      "Validation Loss Error:  1.947, Batch Validation Time: 4 seconds\n",
      "I am getting better as time goes by.\n",
      "Epoch:   2/10, Batch:    0/472, Training Loss Error:  1.492, Training Time on 100 Batches: 18 seconds\n",
      "Epoch:   2/10, Batch:  100/472, Training Loss Error:  1.999, Training Time on 100 Batches: 16 seconds\n",
      "Epoch:   2/10, Batch:  200/472, Training Loss Error:  1.977, Training Time on 100 Batches: 17 seconds\n",
      "Validation Loss Error:  1.917, Batch Validation Time: 4 seconds\n",
      "I am getting better as time goes by.\n",
      "Epoch:   2/10, Batch:  300/472, Training Loss Error:  2.010, Training Time on 100 Batches: 21 seconds\n",
      "Epoch:   2/10, Batch:  400/472, Training Loss Error:  2.054, Training Time on 100 Batches: 22 seconds\n",
      "Validation Loss Error:  1.915, Batch Validation Time: 4 seconds\n",
      "I am getting better as time goes by.\n",
      "Epoch:   3/10, Batch:    0/472, Training Loss Error:  1.452, Training Time on 100 Batches: 26 seconds\n",
      "Epoch:   3/10, Batch:  100/472, Training Loss Error:  1.956, Training Time on 100 Batches: 16 seconds\n",
      "Epoch:   3/10, Batch:  200/472, Training Loss Error:  1.946, Training Time on 100 Batches: 19 seconds\n",
      "Validation Loss Error:  1.904, Batch Validation Time: 4 seconds\n",
      "I am getting better as time goes by.\n",
      "Epoch:   3/10, Batch:  300/472, Training Loss Error:  1.987, Training Time on 100 Batches: 18 seconds\n",
      "Epoch:   3/10, Batch:  400/472, Training Loss Error:  2.036, Training Time on 100 Batches: 25 seconds\n",
      "Validation Loss Error:  1.905, Batch Validation Time: 4 seconds\n",
      "I still need to practice more.\n",
      "Epoch:   4/10, Batch:    0/472, Training Loss Error:  1.440, Training Time on 100 Batches: 18 seconds\n",
      "Epoch:   4/10, Batch:  100/472, Training Loss Error:  1.939, Training Time on 100 Batches: 17 seconds\n",
      "Epoch:   4/10, Batch:  200/472, Training Loss Error:  1.932, Training Time on 100 Batches: 19 seconds\n",
      "Validation Loss Error:  1.894, Batch Validation Time: 4 seconds\n",
      "I am getting better as time goes by.\n",
      "Epoch:   4/10, Batch:  300/472, Training Loss Error:  1.973, Training Time on 100 Batches: 22 seconds\n",
      "Epoch:   4/10, Batch:  400/472, Training Loss Error:  2.021, Training Time on 100 Batches: 18 seconds\n",
      "Validation Loss Error:  1.894, Batch Validation Time: 4 seconds\n",
      "I am getting better as time goes by.\n",
      "Epoch:   5/10, Batch:    0/472, Training Loss Error:  1.430, Training Time on 100 Batches: 17 seconds\n",
      "Epoch:   5/10, Batch:  100/472, Training Loss Error:  1.926, Training Time on 100 Batches: 20 seconds\n",
      "Epoch:   5/10, Batch:  200/472, Training Loss Error:  1.920, Training Time on 100 Batches: 14 seconds\n",
      "Validation Loss Error:  1.883, Batch Validation Time: 4 seconds\n",
      "I am getting better as time goes by.\n",
      "Epoch:   5/10, Batch:  300/472, Training Loss Error:  1.959, Training Time on 100 Batches: 21 seconds\n",
      "Epoch:   5/10, Batch:  400/472, Training Loss Error:  2.007, Training Time on 100 Batches: 21 seconds\n",
      "Validation Loss Error:  1.882, Batch Validation Time: 5 seconds\n",
      "I am getting better as time goes by.\n",
      "Epoch:   6/10, Batch:    0/472, Training Loss Error:  1.423, Training Time on 100 Batches: 20 seconds\n",
      "Epoch:   6/10, Batch:  100/472, Training Loss Error:  1.913, Training Time on 100 Batches: 16 seconds\n",
      "Epoch:   6/10, Batch:  200/472, Training Loss Error:  1.906, Training Time on 100 Batches: 19 seconds\n",
      "Validation Loss Error:  1.863, Batch Validation Time: 4 seconds\n",
      "I am getting better as time goes by.\n",
      "Epoch:   6/10, Batch:  300/472, Training Loss Error:  1.941, Training Time on 100 Batches: 20 seconds\n",
      "Epoch:   6/10, Batch:  400/472, Training Loss Error:  1.990, Training Time on 100 Batches: 25 seconds\n",
      "Validation Loss Error:  1.854, Batch Validation Time: 4 seconds\n",
      "I am getting better as time goes by.\n",
      "Epoch:   7/10, Batch:    0/472, Training Loss Error:  1.403, Training Time on 100 Batches: 16 seconds\n",
      "Epoch:   7/10, Batch:  100/472, Training Loss Error:  1.889, Training Time on 100 Batches: 21 seconds\n",
      "Epoch:   7/10, Batch:  200/472, Training Loss Error:  1.880, Training Time on 100 Batches: 17 seconds\n",
      "Validation Loss Error:  1.834, Batch Validation Time: 3 seconds\n",
      "I am getting better as time goes by.\n",
      "Epoch:   7/10, Batch:  300/472, Training Loss Error:  1.915, Training Time on 100 Batches: 21 seconds\n",
      "Epoch:   7/10, Batch:  400/472, Training Loss Error:  1.961, Training Time on 100 Batches: 17 seconds\n",
      "Validation Loss Error:  1.830, Batch Validation Time: 4 seconds\n",
      "I am getting better as time goes by.\n",
      "Epoch:   8/10, Batch:    0/472, Training Loss Error:  1.388, Training Time on 100 Batches: 19 seconds\n",
      "Epoch:   8/10, Batch:  100/472, Training Loss Error:  1.867, Training Time on 100 Batches: 19 seconds\n",
      "Epoch:   8/10, Batch:  200/472, Training Loss Error:  1.859, Training Time on 100 Batches: 22 seconds\n",
      "Validation Loss Error:  1.816, Batch Validation Time: 4 seconds\n",
      "I am getting better as time goes by.\n",
      "Epoch:   8/10, Batch:  300/472, Training Loss Error:  1.896, Training Time on 100 Batches: 18 seconds\n",
      "Epoch:   8/10, Batch:  400/472, Training Loss Error:  1.943, Training Time on 100 Batches: 23 seconds\n",
      "Validation Loss Error:  1.814, Batch Validation Time: 4 seconds\n",
      "I am getting better as time goes by.\n",
      "Epoch:   9/10, Batch:    0/472, Training Loss Error:  1.377, Training Time on 100 Batches: 15 seconds\n",
      "Epoch:   9/10, Batch:  100/472, Training Loss Error:  1.853, Training Time on 100 Batches: 14 seconds\n",
      "Epoch:   9/10, Batch:  200/472, Training Loss Error:  1.845, Training Time on 100 Batches: 18 seconds\n",
      "Validation Loss Error:  1.804, Batch Validation Time: 4 seconds\n",
      "I am getting better as time goes by.\n",
      "Epoch:   9/10, Batch:  300/472, Training Loss Error:  1.883, Training Time on 100 Batches: 15 seconds\n",
      "Epoch:   9/10, Batch:  400/472, Training Loss Error:  1.931, Training Time on 100 Batches: 19 seconds\n",
      "Validation Loss Error:  1.804, Batch Validation Time: 4 seconds\n",
      "I still need to practice more.\n",
      "Epoch:  10/10, Batch:    0/472, Training Loss Error:  1.367, Training Time on 100 Batches: 16 seconds\n",
      "Epoch:  10/10, Batch:  100/472, Training Loss Error:  1.841, Training Time on 100 Batches: 17 seconds\n",
      "Epoch:  10/10, Batch:  200/472, Training Loss Error:  1.834, Training Time on 100 Batches: 20 seconds\n",
      "Validation Loss Error:  1.793, Batch Validation Time: 4 seconds\n",
      "I am getting better as time goes by.\n",
      "Epoch:  10/10, Batch:  300/472, Training Loss Error:  1.874, Training Time on 100 Batches: 21 seconds\n",
      "Epoch:  10/10, Batch:  400/472, Training Loss Error:  1.919, Training Time on 100 Batches: 20 seconds\n",
      "Validation Loss Error:  1.793, Batch Validation Time: 4 seconds\n",
      "I am getting better as time goes by.\n",
      "Over\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, epochs + 1):\n",
    "    for batch_index, (padded_questions_in_batch, padded_answers_in_batch) in enumerate(split_into_batches(training_questions, training_answers, batch_size)):\n",
    "        starting_time = time.time()\n",
    "        _, batch_training_loss_error = session.run([optimizer_gradient_clipping, loss_error], {inputs: padded_questions_in_batch,\n",
    "                                                                                               targets: padded_answers_in_batch,\n",
    "                                                                                               lr: learning_rate,\n",
    "                                                                                               sequence_length: padded_answers_in_batch.shape[1],\n",
    "                                                                                               keep_prob: keep_probability})\n",
    "        total_training_loss_error += batch_training_loss_error\n",
    "        ending_time = time.time()\n",
    "        batch_time = ending_time - starting_time\n",
    "        if batch_index % batch_index_check_training_loss == 0:\n",
    "            print('Epoch: {:>3}/{}, Batch: {:>4}/{}, Training Loss Error: {:>6.3f}, Training Time on 100 Batches: {:d} seconds'.format(epoch,\n",
    "                                                                                                                                       epochs,\n",
    "                                                                                                                                       batch_index,\n",
    "                                                                                                                                       len(training_questions) // batch_size,\n",
    "                                                                                                                                       total_training_loss_error / batch_index_check_training_loss,\n",
    "                                                                                                                                       int(batch_time * batch_index_check_training_loss)))\n",
    "            total_training_loss_error = 0\n",
    "        if batch_index % batch_index_check_validation_loss == 0 and batch_index > 0:\n",
    "            total_validation_loss_error = 0\n",
    "            starting_time = time.time()\n",
    "            for batch_index_validation, (padded_questions_in_batch, padded_answers_in_batch) in enumerate(split_into_batches(validation_questions, validation_answers, batch_size)):\n",
    "                batch_validation_loss_error = session.run(loss_error, {inputs: padded_questions_in_batch,\n",
    "                                                                       targets: padded_answers_in_batch,\n",
    "                                                                       lr: learning_rate,\n",
    "                                                                       sequence_length: padded_answers_in_batch.shape[1],\n",
    "                                                                       keep_prob: 1})\n",
    "                total_validation_loss_error += batch_validation_loss_error\n",
    "            ending_time = time.time()\n",
    "            batch_time = ending_time - starting_time\n",
    "            average_validation_loss_error = total_validation_loss_error / (len(validation_questions) / batch_size)\n",
    "            print('Validation Loss Error: {:>6.3f}, Batch Validation Time: {:d} seconds'.format(average_validation_loss_error, int(batch_time)))\n",
    "            learning_rate *= learning_rate_decay\n",
    "            if learning_rate < min_learning_rate:\n",
    "                learning_rate = min_learning_rate\n",
    "            list_validation_loss_error.append(average_validation_loss_error)\n",
    "            if average_validation_loss_error <= min(list_validation_loss_error):\n",
    "                print('I am getting better as time goes by.')\n",
    "                early_stopping_check = 0\n",
    "                saver = tf.train.Saver()\n",
    "                saver.save(session, checkpoint)\n",
    "            else:\n",
    "                print(\"I still need to practice more.\")\n",
    "                early_stopping_check += 1\n",
    "                if early_stopping_check == early_stopping_stop:\n",
    "                    break\n",
    "    if early_stopping_check == early_stopping_stop:\n",
    "        print(\"I can't speak any better, this is the best I can do.\")\n",
    "        break\n",
    "print(\"Over\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"./chatbot_weights.ckpt\" # Loading the weights after training from .ckpt file\n",
    "session = tf.InteractiveSession()\n",
    "session.run(tf.global_variables_initializer())\n",
    "saver = tf.train.Saver()\n",
    "saver.restore(session, checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_string2int(question, word2int):\n",
    "    return [word2int.get(word, word2int['<OUT>']) for word in question.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You: okay\n",
      "ChatBot:  I amout.\n",
      "You: oh i do not think so i gave him both barrels\n",
      "ChatBot:  I amout.\n",
      "You: accelerate to mark tommy\n",
      "ChatBot:  I amout.\n",
      "You: fuck you\n",
      "ChatBot:  I do notout.\n",
      "You: fart\n",
      "ChatBot:  I amout.\n",
      "You: not\n",
      "ChatBot:  I do notout.\n",
      "You: yes\n",
      "ChatBot:  Iout.\n",
      "You: hello?\n",
      "ChatBot:  I amout.\n",
      "You: where am I?\n",
      "ChatBot:  I amout.\n",
      "You: who are you\n",
      "ChatBot:  I do not do notout.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    465\u001b[0m         \"\"\"\n\u001b[0;32m--> 466\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-69-63b29181dc17>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwhile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mquestion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mquestion\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'Goodbye'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mquestion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_string2int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquestionswords2int\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m         )\n\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "while(True):\n",
    "    question = input(\"You: \")\n",
    "    if question == 'Goodbye':\n",
    "        break\n",
    "    question = convert_string2int(question, questionswords2int)\n",
    "    question = question + [questionswords2int['<PAD>']] * (25 - len(question))\n",
    "    fake_batch = np.zeros((batch_size, 25))\n",
    "    fake_batch[0] = question\n",
    "    predicted_answer = session.run(test_predictions, {inputs: fake_batch, keep_prob: 0.5})[0]\n",
    "    answer = ''\n",
    "    for i in np.argmax(predicted_answer, 1):\n",
    "        if answersints2word[i] == 'i':\n",
    "            token = ' I'\n",
    "        elif answersints2word[i] == '<EOS>':\n",
    "            token = '.'\n",
    "        elif answersints2word[i] == '<OUT>':\n",
    "            token = 'out'\n",
    "        else:\n",
    "            token = ' ' + answersints2word[i]\n",
    "        answer += token\n",
    "        if token == '.':\n",
    "            break\n",
    "    print('ChatBot: ' + answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "import torch\n",
    "from torch.jit import script, trace\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import csv\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "import unicodedata\n",
    "import codecs\n",
    "from io import open\n",
    "import itertools\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if USE_CUDA else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_name = \"cornell movie-dialogs corpus\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_token = 0  # Used for padding short sentences\n",
    "SOS_token = 1  # Start-of-sentence token\n",
    "EOS_token = 2  # End-of-sentence token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Voc:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.trimmed = False\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
    "        self.num_words = 3  # Count SOS, EOS, PAD\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.num_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.num_words] = word\n",
    "            self.num_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "\n",
    "    # Remove words below a certain count threshold\n",
    "    def trim(self, min_count):\n",
    "        if self.trimmed:\n",
    "            return\n",
    "        self.trimmed = True\n",
    "\n",
    "        keep_words = []\n",
    "\n",
    "        for k, v in self.word2count.items():\n",
    "            if v >= min_count:\n",
    "                keep_words.append(k)\n",
    "\n",
    "        print('keep_words {} / {} = {:.4f}'.format(\n",
    "            len(keep_words), len(self.word2index), len(keep_words) / len(self.word2index)\n",
    "        ))\n",
    "\n",
    "        # Reinitialize dictionaries\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
    "        self.num_words = 3 # Count default tokens\n",
    "\n",
    "        for word in keep_words:\n",
    "            self.addWord(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    s = re.sub(r\"\\s+\", r\" \", s).strip()\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readVocs(data, corpus_name):\n",
    "    print(\"Reading lines...\")\n",
    "    pairs = [[normalizeString(l),normalizeString(h)] for l,h in zip(data.Context.tolist(),data.Utterance.tolist())]\n",
    "    voc = Voc(corpus_name)\n",
    "    return voc, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterPair(p):\n",
    "    # Input sequences need to preserve the last word for EOS token\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and len(p[1].split(' ')) < MAX_LENGTH\n",
    "\n",
    "# Filter pairs using filterPair condition\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadPrepareData(corpus_name, data):\n",
    "    print(\"Start preparing training data ...\")\n",
    "    voc, pairs = readVocs(data, corpus_name)\n",
    "    print(\"Read {!s} sentence pairs\".format(len(pairs)))\n",
    "    pairs = filterPairs(pairs)\n",
    "    print(\"Trimmed to {!s} sentence pairs\".format(len(pairs)))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        voc.addSentence(pair[0])\n",
    "        voc.addSentence(pair[1])\n",
    "    print(\"Counted words:\", voc.num_words)\n",
    "    return voc, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start preparing training data ...\n",
      "Reading lines...\n",
      "Read 20000 sentence pairs\n",
      "Trimmed to 18678 sentence pairs\n",
      "Counting words...\n",
      "Counted words: 10139\n"
     ]
    }
   ],
   "source": [
    "voc, pairs = loadPrepareData(corpus_name, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "pairs:\n",
      "['well i thought we would start with pronunciation if that is okay with you', 'not the hacking and gagging and spitting part please']\n",
      "['not the hacking and gagging and spitting part please', 'okay then how bout we try out some french cuisine saturday night']\n",
      "['you are asking me out that is so cute what is your name again', 'forget it']\n",
      "['no no it is my fault we did not have a proper introduction', 'cameron cameron']\n",
      "['cameron cameron', 'the thing is cameron i am at the mercy of a particularly hideous breed of loser my sister i cannot date until she does']\n",
      "['the thing is cameron i am at the mercy of a particularly hideous breed of loser my sister i cannot date until she does', 'seems like she could get a date easy enough']\n",
      "['why why', 'unsolved mystery she used to be really popular when she started high school then it was just like she got sick of it or something']\n",
      "['unsolved mystery she used to be really popular when she started high school then it was just like she got sick of it or something', 'that is a shame']\n",
      "['gosh if only we could find kat a boyfriend', 'let me see what i can do']\n",
      "['that is because it is such a nice one', 'forget french']\n"
     ]
    }
   ],
   "source": [
    "print(\"\\npairs:\")\n",
    "for pair in pairs[:10]:\n",
    "    print(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_COUNT = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trimRareWords(voc, pairs, MIN_COUNT):\n",
    "    # Trim words used under the MIN_COUNT from the voc\n",
    "    voc.trim(MIN_COUNT)\n",
    "    # Filter out pairs with trimmed words\n",
    "    keep_pairs = []\n",
    "    for pair in pairs:\n",
    "        input_sentence = pair[0]\n",
    "        output_sentence = pair[1]\n",
    "        keep_input = True\n",
    "        keep_output = True\n",
    "        # Check input sentence\n",
    "        for word in input_sentence.split(' '):\n",
    "            if word not in voc.word2index:\n",
    "                keep_input = False\n",
    "                break\n",
    "        # Check output sentence\n",
    "        for word in output_sentence.split(' '):\n",
    "            if word not in voc.word2index:\n",
    "                keep_output = False\n",
    "                break\n",
    "\n",
    "        # Only keep pairs that do not contain trimmed word(s) in their input or output sentence\n",
    "        if keep_input and keep_output:\n",
    "            keep_pairs.append(pair)\n",
    "\n",
    "    print(\"Trimmed from {} pairs to {}, {:.4f} of total\".format(len(pairs), len(keep_pairs), len(keep_pairs) / len(pairs)))\n",
    "    return keep_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keep_words 7628 / 10136 = 0.7526\n",
      "Trimmed from 18678 pairs to 16733, 0.8959 of total\n"
     ]
    }
   ],
   "source": [
    "pairs = trimRareWords(voc, pairs, MIN_COUNT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentence(voc, sentence):\n",
    "    return [voc.word2index[word] for word in sentence.split(' ')] + [EOS_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zeroPadding(l, fillvalue=PAD_token):\n",
    "    return list(itertools.zip_longest(*l, fillvalue=fillvalue))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binaryMatrix(l, value=PAD_token):\n",
    "    m = []\n",
    "    for i, seq in enumerate(l):\n",
    "        m.append([])\n",
    "        for token in seq:\n",
    "            if token == PAD_token:\n",
    "                m[i].append(0)\n",
    "            else:\n",
    "                m[i].append(1)\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inputVar(l, voc):\n",
    "    indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l]\n",
    "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
    "    padList = zeroPadding(indexes_batch)\n",
    "    padVar = torch.LongTensor(padList)\n",
    "    return padVar, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outputVar(l, voc):\n",
    "    indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l]\n",
    "    max_target_len = max([len(indexes) for indexes in indexes_batch])\n",
    "    padList = zeroPadding(indexes_batch)\n",
    "    mask = binaryMatrix(padList)\n",
    "    mask = torch.ByteTensor(mask)\n",
    "    padVar = torch.LongTensor(padList)\n",
    "    return padVar, mask, max_target_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch2TrainData(voc, pair_batch):\n",
    "    pair_batch.sort(key=lambda x: len(x[0].split(\" \")), reverse=True)\n",
    "    input_batch, output_batch = [], []\n",
    "    for pair in pair_batch:\n",
    "        input_batch.append(pair[0])\n",
    "        output_batch.append(pair[1])\n",
    "    inp, lengths = inputVar(input_batch, voc)\n",
    "    output, mask, max_target_len = outputVar(output_batch, voc)\n",
    "    return inp, lengths, output, mask, max_target_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variable: tensor([[  86, 1596,   38,   14,  210],\n",
      "        [ 301,  177,   85,   48, 2162],\n",
      "        [  35, 4038,   65,   49,   43],\n",
      "        [   4,   89,   68,  778,    2],\n",
      "        [ 247,    4,    2,    2,    0],\n",
      "        [ 100,  247,    0,    0,    0],\n",
      "        [ 411, 1609,    0,    0,    0],\n",
      "        [  14, 1853,    0,    0,    0],\n",
      "        [ 229,   11,    0,    0,    0],\n",
      "        [  77, 1321,    0,    0,    0],\n",
      "        [ 100,    2,    0,    0,    0],\n",
      "        [   2,    0,    0,    0,    0]])\n",
      "lengths: tensor([12, 11,  5,  5,  4])\n",
      "target_variable: tensor([[ 787,  155,  272,  438,  306],\n",
      "        [ 787,  306,  272,  301,  141],\n",
      "        [   2,  452,    4,   35, 1432],\n",
      "        [   0,  155,   47,   73,   56],\n",
      "        [   0,  306,   15,   14,  204],\n",
      "        [   0,    2,  168,   33, 5367],\n",
      "        [   0,    0,  142,  248,  325],\n",
      "        [   0,    0, 1050,   11, 1114],\n",
      "        [   0,    0,    3,  340,  135],\n",
      "        [   0,    0,    2, 6805,  128],\n",
      "        [   0,    0,    0, 1874,    6],\n",
      "        [   0,    0,    0,    2, 1125],\n",
      "        [   0,    0,    0,    0,  135],\n",
      "        [   0,    0,    0,    0,  177],\n",
      "        [   0,    0,    0,    0,    2]])\n",
      "mask: tensor([[1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1],\n",
      "        [0, 1, 1, 1, 1],\n",
      "        [0, 1, 1, 1, 1],\n",
      "        [0, 1, 1, 1, 1],\n",
      "        [0, 0, 1, 1, 1],\n",
      "        [0, 0, 1, 1, 1],\n",
      "        [0, 0, 1, 1, 1],\n",
      "        [0, 0, 1, 1, 1],\n",
      "        [0, 0, 0, 1, 1],\n",
      "        [0, 0, 0, 1, 1],\n",
      "        [0, 0, 0, 0, 1],\n",
      "        [0, 0, 0, 0, 1],\n",
      "        [0, 0, 0, 0, 1]], dtype=torch.uint8)\n",
      "max_target_len: 15\n"
     ]
    }
   ],
   "source": [
    "small_batch_size = 5\n",
    "batches = batch2TrainData(voc, [random.choice(pairs) for _ in range(small_batch_size)])\n",
    "input_variable, lengths, target_variable, mask, max_target_len = batches\n",
    "\n",
    "print(\"input_variable:\", input_variable)\n",
    "print(\"lengths:\", lengths)\n",
    "print(\"target_variable:\", target_variable)\n",
    "print(\"mask:\", mask)\n",
    "print(\"max_target_len:\", max_target_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, embedding, n_layers=1, dropout=0):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = embedding\n",
    "\n",
    "        # Initialize GRU; the input_size and hidden_size params are both set to 'hidden_size'\n",
    "        #   because our input size is a word embedding with number of features == hidden_size\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers,\n",
    "                          dropout=(0 if n_layers == 1 else dropout), bidirectional=True)\n",
    "\n",
    "    def forward(self, input_seq, input_lengths, hidden=None):\n",
    "        # Convert word indexes to embeddings\n",
    "        embedded = self.embedding(input_seq)\n",
    "        # Pack padded batch of sequences for RNN module\n",
    "        packed = torch.nn.utils.rnn.pack_padded_sequence(embedded, input_lengths)\n",
    "        # Forward pass through GRU\n",
    "        outputs, hidden = self.gru(packed, hidden)\n",
    "        # Unpack padding\n",
    "        outputs, _ = torch.nn.utils.rnn.pad_packed_sequence(outputs)\n",
    "        # Sum bidirectional GRU outputs\n",
    "        outputs = outputs[:, :, :self.hidden_size] + outputs[:, : ,self.hidden_size:]\n",
    "        # Return output and final hidden state\n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attn(torch.nn.Module):\n",
    "    def __init__(self, method, hidden_size):\n",
    "        super(Attn, self).__init__()\n",
    "        self.method = method\n",
    "        if self.method not in ['dot', 'general', 'concat']:\n",
    "            raise ValueError(self.method, \"is not an appropriate attention method.\")\n",
    "        self.hidden_size = hidden_size\n",
    "        if self.method == 'general':\n",
    "            self.attn = torch.nn.Linear(self.hidden_size, hidden_size)\n",
    "        elif self.method == 'concat':\n",
    "            self.attn = torch.nn.Linear(self.hidden_size * 2, hidden_size)\n",
    "            self.v = torch.nn.Parameter(torch.FloatTensor(hidden_size))\n",
    "\n",
    "    def dot_score(self, hidden, encoder_output):\n",
    "        return torch.sum(hidden * encoder_output, dim=2)\n",
    "\n",
    "    def general_score(self, hidden, encoder_output):\n",
    "        energy = self.attn(encoder_output)\n",
    "        return torch.sum(hidden * energy, dim=2)\n",
    "\n",
    "    def concat_score(self, hidden, encoder_output):\n",
    "        energy = self.attn(torch.cat((hidden.expand(encoder_output.size(0), -1, -1), encoder_output), 2)).tanh()\n",
    "        return torch.sum(self.v * energy, dim=2)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        # Calculate the attention weights (energies) based on the given method\n",
    "        if self.method == 'general':\n",
    "            attn_energies = self.general_score(hidden, encoder_outputs)\n",
    "        elif self.method == 'concat':\n",
    "            attn_energies = self.concat_score(hidden, encoder_outputs)\n",
    "        elif self.method == 'dot':\n",
    "            attn_energies = self.dot_score(hidden, encoder_outputs)\n",
    "\n",
    "        # Transpose max_length and batch_size dimensions\n",
    "        attn_energies = attn_energies.t()\n",
    "\n",
    "        # Return the softmax normalized probability scores (with added dimension)\n",
    "        return F.softmax(attn_energies, dim=1).unsqueeze(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LuongAttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, attn_model, embedding, hidden_size, output_size, n_layers=1, dropout=0.1):\n",
    "        super(LuongAttnDecoderRNN, self).__init__()\n",
    "\n",
    "        # Keep for reference\n",
    "        self.attn_model = attn_model\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Define layers\n",
    "        self.embedding = embedding\n",
    "        self.embedding_dropout = nn.Dropout(dropout)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=(0 if n_layers == 1 else dropout))\n",
    "        self.concat = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "        self.attn = Attn(attn_model, hidden_size)\n",
    "\n",
    "    def forward(self, input_step, last_hidden, encoder_outputs):\n",
    "        # Note: we run this one step (word) at a time\n",
    "        # Get embedding of current input word\n",
    "        embedded = self.embedding(input_step)\n",
    "        embedded = self.embedding_dropout(embedded)\n",
    "        # Forward through unidirectional GRU\n",
    "        rnn_output, hidden = self.gru(embedded, last_hidden)\n",
    "        # Calculate attention weights from the current GRU output\n",
    "        attn_weights = self.attn(rnn_output, encoder_outputs)\n",
    "        # Multiply attention weights to encoder outputs to get new \"weighted sum\" context vector\n",
    "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1))\n",
    "        # Concatenate weighted context vector and GRU output using Luong eq. 5\n",
    "        rnn_output = rnn_output.squeeze(0)\n",
    "        context = context.squeeze(1)\n",
    "        concat_input = torch.cat((rnn_output, context), 1)\n",
    "        concat_output = torch.tanh(self.concat(concat_input))\n",
    "        # Predict next word using Luong eq. 6\n",
    "        output = self.out(concat_output)\n",
    "        output = F.softmax(output, dim=1)\n",
    "        # Return output and final hidden state\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maskNLLLoss(inp, target, mask):\n",
    "    nTotal = mask.sum()\n",
    "    crossEntropy = -torch.log(torch.gather(inp, 1, target.view(-1, 1)))\n",
    "    loss = crossEntropy.masked_select(mask).mean()\n",
    "    loss = loss.to(device)\n",
    "    return loss, nTotal.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(input_variable, lengths, target_variable, mask, max_target_len, encoder, decoder, embedding,\n",
    "          encoder_optimizer, decoder_optimizer, batch_size, clip, max_length=MAX_LENGTH):\n",
    "\n",
    "    # Zero gradients\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    # Set device options\n",
    "    input_variable = input_variable.to(device)\n",
    "    lengths = lengths.to(device)\n",
    "    target_variable = target_variable.to(device)\n",
    "    mask = mask.to(device)\n",
    "\n",
    "    # Initialize variables\n",
    "    loss = 0\n",
    "    print_losses = []\n",
    "    n_totals = 0\n",
    "\n",
    "    # Forward pass through encoder\n",
    "    encoder_outputs, encoder_hidden = encoder(input_variable, lengths)\n",
    "\n",
    "    # Create initial decoder input (start with SOS tokens for each sentence)\n",
    "    decoder_input = torch.LongTensor([[SOS_token for _ in range(batch_size)]])\n",
    "    decoder_input = decoder_input.to(device)\n",
    "\n",
    "    # Set initial decoder hidden state to the encoder's final hidden state\n",
    "    decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
    "\n",
    "    # Determine if we are using teacher forcing this iteration\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    # Forward batch of sequences through decoder one time step at a time\n",
    "    if use_teacher_forcing:\n",
    "        for t in range(max_target_len):\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs\n",
    "            )\n",
    "            # Teacher forcing: next input is current target\n",
    "            decoder_input = target_variable[t].view(1, -1)\n",
    "            # Calculate and accumulate loss\n",
    "            mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t], mask[t])\n",
    "            loss += mask_loss\n",
    "            print_losses.append(mask_loss.item() * nTotal)\n",
    "            n_totals += nTotal\n",
    "    else:\n",
    "        for t in range(max_target_len):\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs\n",
    "            )\n",
    "            # No teacher forcing: next input is decoder's own current output\n",
    "            _, topi = decoder_output.topk(1)\n",
    "            decoder_input = torch.LongTensor([[topi[i][0] for i in range(batch_size)]])\n",
    "            decoder_input = decoder_input.to(device)\n",
    "            # Calculate and accumulate loss\n",
    "            mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t], mask[t])\n",
    "            loss += mask_loss\n",
    "            print_losses.append(mask_loss.item() * nTotal)\n",
    "            n_totals += nTotal\n",
    "\n",
    "    # Perform backpropatation\n",
    "    loss.backward()\n",
    "\n",
    "    # Clip gradients: gradients are modified in place\n",
    "    _ = torch.nn.utils.clip_grad_norm_(encoder.parameters(), clip)\n",
    "    _ = torch.nn.utils.clip_grad_norm_(decoder.parameters(), clip)\n",
    "\n",
    "    # Adjust model weights\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return sum(print_losses) / n_totals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(model_name, voc, pairs, encoder, decoder, encoder_optimizer, decoder_optimizer, embedding, encoder_n_layers, decoder_n_layers, save_dir, n_iteration, batch_size, print_every, save_every, clip, corpus_name, loadFilename):\n",
    "\n",
    "    # Load batches for each iteration\n",
    "    training_batches = [batch2TrainData(voc, [random.choice(pairs) for _ in range(batch_size)])\n",
    "                      for _ in range(n_iteration)]\n",
    "\n",
    "    # Initializations\n",
    "    print('Initializing ...')\n",
    "    start_iteration = 1\n",
    "    print_loss = 0\n",
    "    if loadFilename:\n",
    "        start_iteration = checkpoint['iteration'] + 1\n",
    "\n",
    "    # Training loop\n",
    "    print(\"Training...\")\n",
    "    for iteration in range(start_iteration, n_iteration + 1):\n",
    "        training_batch = training_batches[iteration - 1]\n",
    "        # Extract fields from batch\n",
    "        input_variable, lengths, target_variable, mask, max_target_len = training_batch\n",
    "\n",
    "        # Run a training iteration with batch\n",
    "        loss = train(input_variable, lengths, target_variable, mask, max_target_len, encoder,\n",
    "                     decoder, embedding, encoder_optimizer, decoder_optimizer, batch_size, clip)\n",
    "        print_loss += loss\n",
    "\n",
    "        # Print progress\n",
    "        if iteration % print_every == 0:\n",
    "            print_loss_avg = print_loss / print_every\n",
    "            print(\"Iteration: {}; Percent complete: {:.1f}%; Average loss: {:.4f}\".format(iteration, iteration / n_iteration * 100, print_loss_avg))\n",
    "            print_loss = 0\n",
    "\n",
    "        # Save checkpoint\n",
    "        if (iteration % save_every == 0):\n",
    "            directory = os.path.join(save_dir, model_name, corpus_name, '{}-{}_{}'.format(encoder_n_layers, decoder_n_layers, hidden_size))\n",
    "            if not os.path.exists(directory):\n",
    "                os.makedirs(directory)\n",
    "            torch.save({\n",
    "                'iteration': iteration,\n",
    "                'en': encoder.state_dict(),\n",
    "                'de': decoder.state_dict(),\n",
    "                'en_opt': encoder_optimizer.state_dict(),\n",
    "                'de_opt': decoder_optimizer.state_dict(),\n",
    "                'loss': loss,\n",
    "                'voc_dict': voc.__dict__,\n",
    "                'embedding': embedding.state_dict()\n",
    "            }, os.path.join(directory, '{}_{}.tar'.format(iteration, 'checkpoint')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GreedySearchDecoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(GreedySearchDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, input_seq, input_length, max_length):\n",
    "        # Forward input through encoder model\n",
    "        encoder_outputs, encoder_hidden = self.encoder(input_seq, input_length)\n",
    "        # Prepare encoder's final hidden layer to be first hidden input to the decoder\n",
    "        decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
    "        # Initialize decoder input with SOS_token\n",
    "        decoder_input = torch.ones(1, 1, device=device, dtype=torch.long) * SOS_token\n",
    "        # Initialize tensors to append decoded words to\n",
    "        all_tokens = torch.zeros([0], device=device, dtype=torch.long)\n",
    "        all_scores = torch.zeros([0], device=device)\n",
    "        # Iteratively decode one word token at a time\n",
    "        for _ in range(max_length):\n",
    "            # Forward pass through decoder\n",
    "            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            # Obtain most likely word token and its softmax score\n",
    "            decoder_scores, decoder_input = torch.max(decoder_output, dim=1)\n",
    "            # Record token and score\n",
    "            all_tokens = torch.cat((all_tokens, decoder_input), dim=0)\n",
    "            all_scores = torch.cat((all_scores, decoder_scores), dim=0)\n",
    "            # Prepare current token to be next decoder input (add a dimension)\n",
    "            decoder_input = torch.unsqueeze(decoder_input, 0)\n",
    "        # Return collections of word tokens and scores\n",
    "        return all_tokens, all_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, searcher, voc, sentence, max_length=MAX_LENGTH):\n",
    "    ### Format input sentence as a batch\n",
    "    # words -> indexes\n",
    "    indexes_batch = [indexesFromSentence(voc, sentence)]\n",
    "    # Create lengths tensor\n",
    "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
    "    # Transpose dimensions of batch to match models' expectations\n",
    "    input_batch = torch.LongTensor(indexes_batch).transpose(0, 1)\n",
    "    # Use appropriate device\n",
    "    input_batch = input_batch.to(device)\n",
    "    lengths = lengths.to(device)\n",
    "    # Decode sentence with searcher\n",
    "    tokens, scores = searcher(input_batch, lengths, max_length)\n",
    "    # indexes -> words\n",
    "    decoded_words = [voc.index2word[token.item()] for token in tokens]\n",
    "    return decoded_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateInput(encoder, decoder, searcher, voc):\n",
    "    input_sentence = ''\n",
    "    while(1):\n",
    "        try:\n",
    "            # Get input sentence\n",
    "            input_sentence = input('> ')\n",
    "            # Check if it is quit case\n",
    "            if input_sentence == 'q' or input_sentence == 'quit': break\n",
    "            # Normalize sentence\n",
    "            input_sentence = normalizeString(input_sentence)\n",
    "            # Evaluate sentence\n",
    "            output_words = evaluate(encoder, decoder, searcher, voc, input_sentence)\n",
    "            # Format and print response sentence\n",
    "            output_words[:] = [x for x in output_words if not (x == 'EOS' or x == 'PAD')]\n",
    "            print('Bot:', ' '.join(output_words))\n",
    "\n",
    "        except KeyError:\n",
    "            print(\"Error: Encountered unknown word.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'cb_model'\n",
    "attn_model = 'dot'\n",
    "#attn_model = 'general'\n",
    "#attn_model = 'concat'\n",
    "hidden_size = 500\n",
    "encoder_n_layers = 2\n",
    "decoder_n_layers = 2\n",
    "dropout = 0.1\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "loadFilename = None\n",
    "checkpoint_iter = 4000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "if loadFilename:\n",
    "    # If loading on same machine the model was trained on\n",
    "    checkpoint = torch.load(loadFilename)\n",
    "    # If loading a model trained on GPU to CPU\n",
    "    #checkpoint = torch.load(loadFilename, map_location=torch.device('cpu'))\n",
    "    encoder_sd = checkpoint['en']\n",
    "    decoder_sd = checkpoint['de']\n",
    "    encoder_optimizer_sd = checkpoint['en_opt']\n",
    "    decoder_optimizer_sd = checkpoint['de_opt']\n",
    "    embedding_sd = checkpoint['embedding']\n",
    "    voc.__dict__ = checkpoint['voc_dict']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building encoder and decoder ...\n",
      "Models built and ready to go!\n"
     ]
    }
   ],
   "source": [
    "print('Building encoder and decoder ...')\n",
    "# Initialize word embeddings\n",
    "embedding = nn.Embedding(voc.num_words, hidden_size)\n",
    "if loadFilename:\n",
    "    embedding.load_state_dict(embedding_sd)\n",
    "# Initialize encoder & decoder models\n",
    "encoder = EncoderRNN(hidden_size, embedding, encoder_n_layers, dropout)\n",
    "decoder = LuongAttnDecoderRNN(attn_model, embedding, hidden_size, voc.num_words, decoder_n_layers, dropout)\n",
    "if loadFilename:\n",
    "    encoder.load_state_dict(encoder_sd)\n",
    "    decoder.load_state_dict(decoder_sd)\n",
    "# Use appropriate device\n",
    "encoder = encoder.to(device)\n",
    "decoder = decoder.to(device)\n",
    "print('Models built and ready to go!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip = 50.0\n",
    "teacher_forcing_ratio = 1.0\n",
    "learning_rate = 0.0001\n",
    "decoder_learning_ratio = 5.0\n",
    "n_iteration = 100\n",
    "print_every = 1\n",
    "save_every = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LuongAttnDecoderRNN(\n",
       "  (embedding): Embedding(7631, 500)\n",
       "  (embedding_dropout): Dropout(p=0.1)\n",
       "  (gru): GRU(500, 500, num_layers=2, dropout=0.1)\n",
       "  (concat): Linear(in_features=1000, out_features=500, bias=True)\n",
       "  (out): Linear(in_features=500, out_features=7631, bias=True)\n",
       "  (attn): Attn()\n",
       ")"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.train()\n",
    "decoder.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building optimizers ...\n"
     ]
    }
   ],
   "source": [
    "print('Building optimizers ...')\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate * decoder_learning_ratio)\n",
    "if loadFilename:\n",
    "    encoder_optimizer.load_state_dict(encoder_optimizer_sd)\n",
    "    decoder_optimizer.load_state_dict(decoder_optimizer_sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training!\n",
      "Initializing ...\n",
      "Training...\n",
      "Iteration: 1; Percent complete: 1.0%; Average loss: 5.8095\n",
      "Iteration: 2; Percent complete: 2.0%; Average loss: 5.6306\n",
      "Iteration: 3; Percent complete: 3.0%; Average loss: 5.6380\n",
      "Iteration: 4; Percent complete: 4.0%; Average loss: 5.2660\n",
      "Iteration: 5; Percent complete: 5.0%; Average loss: 4.8170\n",
      "Iteration: 6; Percent complete: 6.0%; Average loss: 4.7903\n",
      "Iteration: 7; Percent complete: 7.0%; Average loss: 4.9006\n",
      "Iteration: 8; Percent complete: 8.0%; Average loss: 4.6920\n",
      "Iteration: 9; Percent complete: 9.0%; Average loss: 4.3203\n",
      "Iteration: 10; Percent complete: 10.0%; Average loss: 4.3323\n",
      "Iteration: 11; Percent complete: 11.0%; Average loss: 4.2064\n",
      "Iteration: 12; Percent complete: 12.0%; Average loss: 4.0289\n",
      "Iteration: 13; Percent complete: 13.0%; Average loss: 4.2022\n",
      "Iteration: 14; Percent complete: 14.0%; Average loss: 4.0798\n",
      "Iteration: 15; Percent complete: 15.0%; Average loss: 3.8068\n",
      "Iteration: 16; Percent complete: 16.0%; Average loss: 3.8924\n",
      "Iteration: 17; Percent complete: 17.0%; Average loss: 3.8123\n",
      "Iteration: 18; Percent complete: 18.0%; Average loss: 3.7983\n",
      "Iteration: 19; Percent complete: 19.0%; Average loss: 3.8151\n",
      "Iteration: 20; Percent complete: 20.0%; Average loss: 3.8696\n",
      "Iteration: 21; Percent complete: 21.0%; Average loss: 3.7507\n",
      "Iteration: 22; Percent complete: 22.0%; Average loss: 3.8838\n",
      "Iteration: 23; Percent complete: 23.0%; Average loss: 3.6621\n",
      "Iteration: 24; Percent complete: 24.0%; Average loss: 3.5944\n",
      "Iteration: 25; Percent complete: 25.0%; Average loss: 3.6994\n",
      "Iteration: 26; Percent complete: 26.0%; Average loss: 3.8277\n",
      "Iteration: 27; Percent complete: 27.0%; Average loss: 4.0318\n",
      "Iteration: 28; Percent complete: 28.0%; Average loss: 3.5871\n",
      "Iteration: 29; Percent complete: 29.0%; Average loss: 3.7390\n",
      "Iteration: 30; Percent complete: 30.0%; Average loss: 3.5833\n",
      "Iteration: 31; Percent complete: 31.0%; Average loss: 3.8681\n",
      "Iteration: 32; Percent complete: 32.0%; Average loss: 4.1150\n",
      "Iteration: 33; Percent complete: 33.0%; Average loss: 3.5022\n",
      "Iteration: 34; Percent complete: 34.0%; Average loss: 3.8911\n",
      "Iteration: 35; Percent complete: 35.0%; Average loss: 3.7139\n",
      "Iteration: 36; Percent complete: 36.0%; Average loss: 3.6925\n",
      "Iteration: 37; Percent complete: 37.0%; Average loss: 3.5739\n",
      "Iteration: 38; Percent complete: 38.0%; Average loss: 3.8723\n",
      "Iteration: 39; Percent complete: 39.0%; Average loss: 3.7135\n",
      "Iteration: 40; Percent complete: 40.0%; Average loss: 3.6833\n",
      "Iteration: 41; Percent complete: 41.0%; Average loss: 3.6516\n",
      "Iteration: 42; Percent complete: 42.0%; Average loss: 3.5367\n",
      "Iteration: 43; Percent complete: 43.0%; Average loss: 3.5130\n",
      "Iteration: 44; Percent complete: 44.0%; Average loss: 3.7767\n",
      "Iteration: 45; Percent complete: 45.0%; Average loss: 3.4486\n",
      "Iteration: 46; Percent complete: 46.0%; Average loss: 3.6444\n",
      "Iteration: 47; Percent complete: 47.0%; Average loss: 3.5831\n",
      "Iteration: 48; Percent complete: 48.0%; Average loss: 3.6482\n",
      "Iteration: 49; Percent complete: 49.0%; Average loss: 3.4994\n",
      "Iteration: 50; Percent complete: 50.0%; Average loss: 3.6985\n",
      "Iteration: 51; Percent complete: 51.0%; Average loss: 3.3539\n",
      "Iteration: 52; Percent complete: 52.0%; Average loss: 3.8638\n",
      "Iteration: 53; Percent complete: 53.0%; Average loss: 3.4821\n",
      "Iteration: 54; Percent complete: 54.0%; Average loss: 3.8619\n",
      "Iteration: 55; Percent complete: 55.0%; Average loss: 3.8637\n",
      "Iteration: 56; Percent complete: 56.0%; Average loss: 3.5024\n",
      "Iteration: 57; Percent complete: 57.0%; Average loss: 3.5340\n",
      "Iteration: 58; Percent complete: 58.0%; Average loss: 3.6697\n",
      "Iteration: 59; Percent complete: 59.0%; Average loss: 3.5326\n",
      "Iteration: 60; Percent complete: 60.0%; Average loss: 3.5797\n",
      "Iteration: 61; Percent complete: 61.0%; Average loss: 3.8943\n",
      "Iteration: 62; Percent complete: 62.0%; Average loss: 3.5745\n",
      "Iteration: 63; Percent complete: 63.0%; Average loss: 3.7179\n",
      "Iteration: 64; Percent complete: 64.0%; Average loss: 3.5627\n",
      "Iteration: 65; Percent complete: 65.0%; Average loss: 3.6384\n",
      "Iteration: 66; Percent complete: 66.0%; Average loss: 3.8113\n",
      "Iteration: 67; Percent complete: 67.0%; Average loss: 3.5801\n",
      "Iteration: 68; Percent complete: 68.0%; Average loss: 3.2757\n",
      "Iteration: 69; Percent complete: 69.0%; Average loss: 3.2166\n",
      "Iteration: 70; Percent complete: 70.0%; Average loss: 3.5014\n",
      "Iteration: 71; Percent complete: 71.0%; Average loss: 3.3306\n",
      "Iteration: 72; Percent complete: 72.0%; Average loss: 3.5881\n",
      "Iteration: 73; Percent complete: 73.0%; Average loss: 3.4259\n",
      "Iteration: 74; Percent complete: 74.0%; Average loss: 3.4321\n",
      "Iteration: 75; Percent complete: 75.0%; Average loss: 3.3264\n",
      "Iteration: 76; Percent complete: 76.0%; Average loss: 3.6137\n",
      "Iteration: 77; Percent complete: 77.0%; Average loss: 3.5227\n",
      "Iteration: 78; Percent complete: 78.0%; Average loss: 3.3464\n",
      "Iteration: 79; Percent complete: 79.0%; Average loss: 3.5046\n",
      "Iteration: 80; Percent complete: 80.0%; Average loss: 3.7052\n",
      "Iteration: 81; Percent complete: 81.0%; Average loss: 3.4515\n",
      "Iteration: 82; Percent complete: 82.0%; Average loss: 3.8377\n",
      "Iteration: 83; Percent complete: 83.0%; Average loss: 3.3852\n",
      "Iteration: 84; Percent complete: 84.0%; Average loss: 3.5469\n",
      "Iteration: 85; Percent complete: 85.0%; Average loss: 3.8545\n",
      "Iteration: 86; Percent complete: 86.0%; Average loss: 3.3606\n",
      "Iteration: 87; Percent complete: 87.0%; Average loss: 3.5349\n",
      "Iteration: 88; Percent complete: 88.0%; Average loss: 3.1995\n",
      "Iteration: 89; Percent complete: 89.0%; Average loss: 3.3619\n",
      "Iteration: 90; Percent complete: 90.0%; Average loss: 3.3393\n",
      "Iteration: 91; Percent complete: 91.0%; Average loss: 3.4895\n",
      "Iteration: 92; Percent complete: 92.0%; Average loss: 3.4750\n",
      "Iteration: 93; Percent complete: 93.0%; Average loss: 3.8468\n",
      "Iteration: 94; Percent complete: 94.0%; Average loss: 3.3831\n",
      "Iteration: 95; Percent complete: 95.0%; Average loss: 3.5842\n",
      "Iteration: 96; Percent complete: 96.0%; Average loss: 3.1957\n",
      "Iteration: 97; Percent complete: 97.0%; Average loss: 3.2422\n",
      "Iteration: 98; Percent complete: 98.0%; Average loss: 3.2861\n",
      "Iteration: 99; Percent complete: 99.0%; Average loss: 3.3694\n",
      "Iteration: 100; Percent complete: 100.0%; Average loss: 3.6530\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting Training!\")\n",
    "trainIters(model_name, voc, pairs, encoder, decoder, encoder_optimizer, decoder_optimizer,\n",
    "           embedding, encoder_n_layers, decoder_n_layers, save_dir, n_iteration, batch_size,\n",
    "           print_every, save_every, clip, corpus_name, loadFilename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LuongAttnDecoderRNN(\n",
       "  (embedding): Embedding(7631, 500)\n",
       "  (embedding_dropout): Dropout(p=0.1)\n",
       "  (gru): GRU(500, 500, num_layers=2, dropout=0.1)\n",
       "  (concat): Linear(in_features=1000, out_features=500, bias=True)\n",
       "  (out): Linear(in_features=500, out_features=7631, bias=True)\n",
       "  (attn): Attn()\n",
       ")"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.eval()\n",
    "decoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "searcher = GreedySearchDecoder(encoder, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GreedySearchDecoder(\n",
       "  (encoder): EncoderRNN(\n",
       "    (embedding): Embedding(7631, 500)\n",
       "    (gru): GRU(500, 500, num_layers=2, dropout=0.1, bidirectional=True)\n",
       "  )\n",
       "  (decoder): LuongAttnDecoderRNN(\n",
       "    (embedding): Embedding(7631, 500)\n",
       "    (embedding_dropout): Dropout(p=0.1)\n",
       "    (gru): GRU(500, 500, num_layers=2, dropout=0.1)\n",
       "    (concat): Linear(in_features=1000, out_features=500, bias=True)\n",
       "    (out): Linear(in_features=500, out_features=7631, bias=True)\n",
       "    (attn): Attn()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "searcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Thanks \n",
      "Bot: i do not know you\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    465\u001b[0m         \"\"\"\n\u001b[0;32m--> 466\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-212-4e20b9561b77>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mevaluateInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msearcher\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-192-afe5eb9c41e9>\u001b[0m in \u001b[0;36mevaluateInput\u001b[0;34m(encoder, decoder, searcher, voc)\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0;31m# Get input sentence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m             \u001b[0minput_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'> '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m             \u001b[0;31m# Check if it is quit case\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minput_sentence\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'q'\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0minput_sentence\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'quit'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m         )\n\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "evaluateInput(encoder, decoder, searcher, voc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
