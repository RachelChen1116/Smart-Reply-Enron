{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers import Dense, Input, Embedding\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from wordcloud import WordCloud\n",
    "import gensim\n",
    "from gensim.models import word2vec\n",
    "import logging\n",
    "import  tensorflow as tf\n",
    "from collections import Counter\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import urllib.request\n",
    "import os\n",
    "import sys\n",
    "import zipfile\n",
    "import logging\n",
    "import pydot\n",
    "import graphviz\n",
    "import re\n",
    "import re\n",
    "from functools import reduce\n",
    "from collections import defaultdict\n",
    "import collections\n",
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "import wordsegment\n",
    "from wordsegment import load, segment\n",
    "from operator import itemgetter\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.fillna(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('vocabulary.txt', 'r') as f:\n",
    "    s = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = s.strip().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = [s0.strip() for s0 in s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['!',\n",
       " \"'\",\n",
       " ',',\n",
       " '.',\n",
       " '?',\n",
       " 'a',\n",
       " 'aa',\n",
       " 'aaa',\n",
       " 'aaaa',\n",
       " 'aaaaaaaaaaaaaaaahhhhhhhh',\n",
       " 'aaaaaaaaaaaahhhhhhhhhhhh',\n",
       " 'aaron',\n",
       " 'ab',\n",
       " 'aba',\n",
       " 'abandon',\n",
       " 'abandoned',\n",
       " 'abandoning',\n",
       " 'abandonment',\n",
       " 'abating',\n",
       " 'abb',\n",
       " 'abba',\n",
       " 'abbreviated',\n",
       " 'abbs',\n",
       " 'abc',\n",
       " 'abcsports',\n",
       " 'abd',\n",
       " 'abel',\n",
       " 'abernathy',\n",
       " 'abg',\n",
       " 'abhijeet',\n",
       " 'abides',\n",
       " 'abigail',\n",
       " 'abilities',\n",
       " 'ability',\n",
       " 'abitibi',\n",
       " 'abl',\n",
       " 'able',\n",
       " 'ablity',\n",
       " 'ably',\n",
       " 'abn',\n",
       " 'abnormality',\n",
       " 'abo',\n",
       " 'aboard',\n",
       " 'about',\n",
       " 'above',\n",
       " 'abrade',\n",
       " 'abramo',\n",
       " 'abrams',\n",
       " 'abreast',\n",
       " 'abri',\n",
       " 'abroad',\n",
       " 'abruptly',\n",
       " 'abs',\n",
       " 'absence',\n",
       " 'absences',\n",
       " 'absense',\n",
       " 'absent',\n",
       " 'abshire',\n",
       " 'absolute',\n",
       " 'absolutely',\n",
       " 'absolutley',\n",
       " 'absolve',\n",
       " 'absorb',\n",
       " 'absorbed',\n",
       " 'absorbs',\n",
       " 'abstraction',\n",
       " 'absurd',\n",
       " 'absurdity',\n",
       " 'abt',\n",
       " 'abu',\n",
       " 'abundantly',\n",
       " 'abuse',\n",
       " 'abused',\n",
       " 'abx',\n",
       " 'ac',\n",
       " 'aca',\n",
       " 'acacia',\n",
       " 'academic',\n",
       " 'academy',\n",
       " 'acc',\n",
       " 'acce',\n",
       " 'accelerate',\n",
       " 'accelerated',\n",
       " 'accelerating',\n",
       " 'acceleration',\n",
       " 'accellera',\n",
       " 'accent',\n",
       " 'accenture',\n",
       " 'accept',\n",
       " 'acceptability',\n",
       " 'acceptable',\n",
       " 'acceptance',\n",
       " 'acceptances',\n",
       " 'accepted',\n",
       " 'accepting',\n",
       " 'accepts',\n",
       " 'access',\n",
       " 'accessible',\n",
       " 'accessing',\n",
       " 'accessories',\n",
       " 'accessory',\n",
       " 'accident',\n",
       " 'accidentally',\n",
       " 'accidently',\n",
       " 'accidents',\n",
       " 'accommodate',\n",
       " 'accommodated',\n",
       " 'accommodates',\n",
       " 'accommodating',\n",
       " 'accomodate',\n",
       " 'accomodates',\n",
       " 'accomodations',\n",
       " 'accompanied',\n",
       " 'accompany',\n",
       " 'accompanying',\n",
       " 'accomplish',\n",
       " 'accomplished',\n",
       " 'accomplishing',\n",
       " 'accomplishment',\n",
       " 'accomplishments',\n",
       " 'accord',\n",
       " 'accordance',\n",
       " 'accorded',\n",
       " 'according',\n",
       " 'accordingly',\n",
       " 'accoun',\n",
       " 'account',\n",
       " 'accountability',\n",
       " 'accountant',\n",
       " 'accountants',\n",
       " 'accounted',\n",
       " 'accounting',\n",
       " 'accounts',\n",
       " 'accross',\n",
       " 'accrual',\n",
       " 'accruals',\n",
       " 'accrue',\n",
       " 'accrued',\n",
       " 'acct',\n",
       " 'acctg',\n",
       " 'accu',\n",
       " 'accum',\n",
       " 'accumulated',\n",
       " 'accura',\n",
       " 'accuracy',\n",
       " 'accurate',\n",
       " 'accurately',\n",
       " 'accusations',\n",
       " 'accused',\n",
       " 'ace',\n",
       " 'acero',\n",
       " 'acess',\n",
       " 'ach',\n",
       " 'ache',\n",
       " 'ached',\n",
       " 'acheive',\n",
       " 'achievable',\n",
       " 'achieve',\n",
       " 'achieved',\n",
       " 'achievement',\n",
       " 'aching',\n",
       " 'achr',\n",
       " 'aci',\n",
       " 'acient',\n",
       " 'ack',\n",
       " 'acker',\n",
       " 'acknowledge',\n",
       " 'acknowledged',\n",
       " 'acknowledgement',\n",
       " 'acknowledges',\n",
       " 'acknowledging',\n",
       " 'acknowledgment',\n",
       " 'aclu',\n",
       " 'acm',\n",
       " 'acn',\n",
       " 'acosta',\n",
       " 'acquire',\n",
       " 'acquired',\n",
       " 'acquires',\n",
       " 'acquiring',\n",
       " 'acquisition',\n",
       " 'acquisitions',\n",
       " 'acquistion',\n",
       " 'acr',\n",
       " 'acre',\n",
       " 'acreage',\n",
       " 'acres',\n",
       " 'acrobat',\n",
       " 'across',\n",
       " 'acsec',\n",
       " 'act',\n",
       " 'acted',\n",
       " 'acting',\n",
       " 'actio',\n",
       " 'action',\n",
       " 'actions',\n",
       " 'activate',\n",
       " 'activating',\n",
       " 'activation',\n",
       " 'activations',\n",
       " 'active',\n",
       " 'actively',\n",
       " 'activist',\n",
       " 'activities',\n",
       " 'activity',\n",
       " 'acts',\n",
       " 'actual',\n",
       " 'actualization',\n",
       " 'actualized',\n",
       " 'actually',\n",
       " 'actuals',\n",
       " 'actuarial',\n",
       " 'actuate',\n",
       " 'actuator',\n",
       " 'acumen',\n",
       " 'ad',\n",
       " 'ada',\n",
       " 'adam',\n",
       " 'adamant',\n",
       " 'adamek',\n",
       " 'adams',\n",
       " 'adamson',\n",
       " 'adapt',\n",
       " 'adaptability',\n",
       " 'adaptations',\n",
       " 'adapting',\n",
       " 'adarsh',\n",
       " 'add',\n",
       " 'added',\n",
       " 'addendum',\n",
       " 'adding',\n",
       " 'addition',\n",
       " 'additional',\n",
       " 'additionally',\n",
       " 'additions',\n",
       " 'additive',\n",
       " 'additonal',\n",
       " 'addres',\n",
       " 'address',\n",
       " 'addressed',\n",
       " 'addressee',\n",
       " 'addresses',\n",
       " 'addressing',\n",
       " 'adds',\n",
       " 'adduser',\n",
       " 'addy',\n",
       " 'ade',\n",
       " 'adecco',\n",
       " 'adele',\n",
       " 'adequacy',\n",
       " 'adequate',\n",
       " 'adequately',\n",
       " 'adhere',\n",
       " 'adhered',\n",
       " 'adhering',\n",
       " 'adios',\n",
       " 'aditya',\n",
       " 'adj',\n",
       " 'adjacent',\n",
       " 'adjourn',\n",
       " 'adjourns',\n",
       " 'adjudicated',\n",
       " 'adjust',\n",
       " 'adjusted',\n",
       " 'adjuster',\n",
       " 'adjusting',\n",
       " 'adjustment',\n",
       " 'adjustments',\n",
       " 'adl',\n",
       " 'adleman',\n",
       " 'adm',\n",
       " 'admin',\n",
       " 'administer',\n",
       " 'administered',\n",
       " 'administering',\n",
       " 'administers',\n",
       " 'administration',\n",
       " 'administrations',\n",
       " 'administrative',\n",
       " 'administratively',\n",
       " 'administrator',\n",
       " 'administrators',\n",
       " 'admins',\n",
       " 'adminstrator',\n",
       " 'admire',\n",
       " 'admired',\n",
       " 'admission',\n",
       " 'admissions',\n",
       " 'admit',\n",
       " 'admits',\n",
       " 'admitted',\n",
       " 'admonition',\n",
       " 'ado',\n",
       " 'adobe',\n",
       " 'adoni',\n",
       " 'adopt',\n",
       " 'adopted',\n",
       " 'adoptees',\n",
       " 'adopting',\n",
       " 'adoption',\n",
       " 'adopts',\n",
       " 'adorable',\n",
       " 'adore',\n",
       " 'adr',\n",
       " 'adresses',\n",
       " 'adrian',\n",
       " 'adriana',\n",
       " 'adrianne',\n",
       " 'ads',\n",
       " 'adsl',\n",
       " 'adult',\n",
       " 'adults',\n",
       " 'adv',\n",
       " 'advance',\n",
       " 'advanced',\n",
       " 'advances',\n",
       " 'advancing',\n",
       " 'advantage',\n",
       " 'advantageous',\n",
       " 'advantages',\n",
       " 'adve',\n",
       " 'adventure',\n",
       " 'adversary',\n",
       " 'adverse',\n",
       " 'adversely',\n",
       " 'advert',\n",
       " 'advertised',\n",
       " 'advertisement',\n",
       " 'advertisers',\n",
       " 'advertising',\n",
       " 'advice',\n",
       " 'advise',\n",
       " 'advised',\n",
       " 'advising',\n",
       " 'advisor',\n",
       " 'advisors',\n",
       " 'advisory',\n",
       " 'advocate',\n",
       " 'advocated',\n",
       " 'advocates',\n",
       " 'advocating',\n",
       " 'advogados',\n",
       " 'ae',\n",
       " 'aeb',\n",
       " 'aec',\n",
       " 'aee',\n",
       " 'aegis',\n",
       " 'aei',\n",
       " 'aep',\n",
       " 'aepi',\n",
       " 'aero',\n",
       " 'aes',\n",
       " 'aeub',\n",
       " 'afb',\n",
       " 'aff',\n",
       " 'affairs',\n",
       " 'affect',\n",
       " 'affected',\n",
       " 'affecting',\n",
       " 'affectionate',\n",
       " 'affects',\n",
       " 'affi',\n",
       " 'affidavit',\n",
       " 'affidavits',\n",
       " 'affil',\n",
       " 'affiliate',\n",
       " 'affiliates',\n",
       " 'affirm',\n",
       " 'affirmative',\n",
       " 'affirmed',\n",
       " 'affirming',\n",
       " 'afford',\n",
       " 'affordable',\n",
       " 'afforded',\n",
       " 'afgan',\n",
       " 'afghan',\n",
       " 'afghanistan',\n",
       " 'afi',\n",
       " 'aforementioned',\n",
       " 'afraid',\n",
       " 'africa',\n",
       " 'african',\n",
       " 'aft',\n",
       " 'afte',\n",
       " 'after',\n",
       " 'afternoon',\n",
       " 'afternoons',\n",
       " 'aftershock',\n",
       " 'afterward',\n",
       " 'afterwards',\n",
       " 'afu',\n",
       " 'ag',\n",
       " 'aga',\n",
       " 'again',\n",
       " 'against',\n",
       " 'agarwal',\n",
       " 'age',\n",
       " 'ageing',\n",
       " 'agencies',\n",
       " 'agency',\n",
       " 'agenda',\n",
       " 'agendas',\n",
       " 'agent',\n",
       " 'agents',\n",
       " 'ages',\n",
       " 'agg',\n",
       " 'aggie',\n",
       " 'aggies',\n",
       " 'aggre',\n",
       " 'aggregate',\n",
       " 'aggregated',\n",
       " 'aggregating',\n",
       " 'aggregation',\n",
       " 'aggregator',\n",
       " 'aggregators',\n",
       " 'aggresive',\n",
       " 'aggressive',\n",
       " 'aggressively',\n",
       " 'aggressiveness',\n",
       " 'aggro',\n",
       " 'agility',\n",
       " 'aging',\n",
       " 'agni',\n",
       " 'agnostic',\n",
       " 'ago',\n",
       " 'agr',\n",
       " 'agre',\n",
       " 'agree',\n",
       " 'agreeable',\n",
       " 'agreed',\n",
       " 'agreeing',\n",
       " 'agreement',\n",
       " 'agreements',\n",
       " 'agrees',\n",
       " 'agressive',\n",
       " 'agricultural',\n",
       " 'ags',\n",
       " 'agt',\n",
       " 'aguilar',\n",
       " 'ah',\n",
       " 'aha',\n",
       " 'ahc',\n",
       " 'ahead',\n",
       " 'ahern',\n",
       " 'ahhh',\n",
       " 'ahhhh',\n",
       " 'ahmed',\n",
       " 'ahn',\n",
       " 'ahoy',\n",
       " 'ahve',\n",
       " 'aia',\n",
       " 'aib',\n",
       " 'aichi',\n",
       " 'aid',\n",
       " 'aide',\n",
       " 'aids',\n",
       " 'aig',\n",
       " 'ailable',\n",
       " 'ails',\n",
       " 'aim',\n",
       " 'aimee',\n",
       " 'aiming',\n",
       " 'aims',\n",
       " 'ain',\n",
       " 'ainsley',\n",
       " 'air',\n",
       " 'airborne',\n",
       " 'aircraft',\n",
       " 'aired',\n",
       " 'airfare',\n",
       " 'airline',\n",
       " 'airlines',\n",
       " 'airmail',\n",
       " 'airplane',\n",
       " 'airport',\n",
       " 'aish',\n",
       " 'ajilon',\n",
       " 'ajr',\n",
       " 'ak',\n",
       " 'aka',\n",
       " 'akamai',\n",
       " 'akers',\n",
       " 'akiko',\n",
       " 'akin',\n",
       " 'al',\n",
       " 'ala',\n",
       " 'alabama',\n",
       " 'aladdin',\n",
       " 'alain',\n",
       " 'alamo',\n",
       " 'alan',\n",
       " 'alarm',\n",
       " 'alaska',\n",
       " 'alaskan',\n",
       " 'albany',\n",
       " 'albert',\n",
       " 'alberta',\n",
       " 'alberto',\n",
       " 'albrecht',\n",
       " 'album',\n",
       " 'albuquerque',\n",
       " 'alchemy',\n",
       " 'alcoa',\n",
       " 'alcohol',\n",
       " 'alcoholic',\n",
       " 'ald',\n",
       " 'alder',\n",
       " 'aldo',\n",
       " 'aleck',\n",
       " 'alejandra',\n",
       " 'alejandro',\n",
       " 'alert',\n",
       " 'alerted',\n",
       " 'alerting',\n",
       " 'alerts',\n",
       " 'alex',\n",
       " 'alexan',\n",
       " 'alexander',\n",
       " 'alexia',\n",
       " 'alexis',\n",
       " 'alf',\n",
       " 'alfa',\n",
       " 'alford',\n",
       " 'alfred',\n",
       " 'alfredo',\n",
       " 'algas',\n",
       " 'alger',\n",
       " 'algore',\n",
       " 'algorithm',\n",
       " 'algorithms',\n",
       " 'ali',\n",
       " 'alia',\n",
       " 'aliases',\n",
       " 'alice',\n",
       " 'alien',\n",
       " 'align',\n",
       " 'alignment',\n",
       " 'alik',\n",
       " 'alike',\n",
       " 'alina',\n",
       " 'alisa',\n",
       " 'alisha',\n",
       " 'alison',\n",
       " 'alive',\n",
       " 'alj',\n",
       " 'aljs',\n",
       " 'all',\n",
       " 'alla',\n",
       " 'allan',\n",
       " 'allay',\n",
       " 'allegation',\n",
       " 'allegations',\n",
       " 'allege',\n",
       " 'alleged',\n",
       " 'alleghany',\n",
       " 'allegheny',\n",
       " 'allegretti',\n",
       " 'allen',\n",
       " 'allergic',\n",
       " 'allergies',\n",
       " 'allergist',\n",
       " 'alleviate',\n",
       " 'alleviating',\n",
       " 'alliance',\n",
       " 'alliances',\n",
       " 'alliant',\n",
       " 'allie',\n",
       " 'allison',\n",
       " 'alloc',\n",
       " 'allocate',\n",
       " 'allocated',\n",
       " 'allocating',\n",
       " 'allocation',\n",
       " 'allocations',\n",
       " 'allot',\n",
       " 'allow',\n",
       " 'allowable',\n",
       " 'allowance',\n",
       " 'allowances',\n",
       " 'allowed',\n",
       " 'allowing',\n",
       " 'allows',\n",
       " 'alloy',\n",
       " 'allusion',\n",
       " 'ally',\n",
       " 'alma',\n",
       " 'almaty',\n",
       " 'almeida',\n",
       " 'almond',\n",
       " 'almost',\n",
       " 'alo',\n",
       " 'alone',\n",
       " 'along',\n",
       " 'alongside',\n",
       " 'alonso',\n",
       " 'alpha',\n",
       " 'alphabetical',\n",
       " 'alport',\n",
       " 'already',\n",
       " 'alreay',\n",
       " 'alright',\n",
       " 'alsi',\n",
       " 'also',\n",
       " 'alstom',\n",
       " 'alstott',\n",
       " 'alt',\n",
       " 'alter',\n",
       " 'alterations',\n",
       " 'altered',\n",
       " 'alternate',\n",
       " 'alternates',\n",
       " 'alternative',\n",
       " 'alternatively',\n",
       " 'alternatives',\n",
       " 'alth',\n",
       " 'altho',\n",
       " 'although',\n",
       " 'altman',\n",
       " 'alto',\n",
       " 'altom',\n",
       " 'alue',\n",
       " 'alum',\n",
       " 'aluminium',\n",
       " 'aluminum',\n",
       " 'alumni',\n",
       " 'alvarado',\n",
       " 'alvarez',\n",
       " 'alvin',\n",
       " 'alway',\n",
       " 'always',\n",
       " 'alzheimers',\n",
       " 'am',\n",
       " 'amaco',\n",
       " 'amador',\n",
       " 'amalgamation',\n",
       " 'amanda',\n",
       " 'amarillo',\n",
       " 'amateurs',\n",
       " 'amaze',\n",
       " 'amazed',\n",
       " 'amazing',\n",
       " 'amazon',\n",
       " 'amber',\n",
       " 'ambiguities',\n",
       " 'ambiguity',\n",
       " 'ambitious',\n",
       " 'ambulance',\n",
       " 'amd',\n",
       " 'ame',\n",
       " 'amelia',\n",
       " 'amelio',\n",
       " 'amen',\n",
       " 'amenable',\n",
       " 'amend',\n",
       " 'amended',\n",
       " 'amending',\n",
       " 'amendment',\n",
       " 'amendments',\n",
       " 'amends',\n",
       " 'amerada',\n",
       " 'ameren',\n",
       " 'amerex',\n",
       " 'ameri',\n",
       " 'americ',\n",
       " 'america',\n",
       " 'american',\n",
       " 'americans',\n",
       " 'americas',\n",
       " 'amerigas',\n",
       " 'ames',\n",
       " 'amex',\n",
       " 'ami',\n",
       " 'amicus',\n",
       " 'amie',\n",
       " 'amigo',\n",
       " 'amil',\n",
       " 'amirault',\n",
       " 'amita',\n",
       " 'ammendment',\n",
       " 'ammo',\n",
       " 'ammonia',\n",
       " 'ammunition',\n",
       " 'amo',\n",
       " 'amoco',\n",
       " 'among',\n",
       " 'amongst',\n",
       " 'amortization',\n",
       " 'amortize',\n",
       " 'amortized',\n",
       " 'amory',\n",
       " 'amount',\n",
       " 'amounted',\n",
       " 'amounts',\n",
       " 'amp',\n",
       " 'ample',\n",
       " 'amps',\n",
       " 'amr',\n",
       " 'amro',\n",
       " 'ams',\n",
       " 'amt',\n",
       " 'amusing',\n",
       " 'amy',\n",
       " 'an',\n",
       " 'ana',\n",
       " 'anabella',\n",
       " 'anabelle',\n",
       " 'anadarko',\n",
       " 'anai',\n",
       " 'anal',\n",
       " 'analog',\n",
       " 'analogy',\n",
       " 'analyis',\n",
       " 'analys',\n",
       " 'analyses',\n",
       " 'analysis',\n",
       " 'analyst',\n",
       " 'analysts',\n",
       " 'analytic',\n",
       " 'analytical',\n",
       " 'analytics',\n",
       " 'analyze',\n",
       " 'analyzed',\n",
       " 'analyzer',\n",
       " 'analyzing',\n",
       " 'anaylsis',\n",
       " 'ance',\n",
       " 'ancestral',\n",
       " 'anchor',\n",
       " 'ancic',\n",
       " 'ancillaries',\n",
       " 'ancillary',\n",
       " 'and',\n",
       " 'andersen',\n",
       " 'anderson',\n",
       " 'andre',\n",
       " 'andrea',\n",
       " 'andreia',\n",
       " 'andrew',\n",
       " 'andrews',\n",
       " 'andrist',\n",
       " 'andromeda',\n",
       " 'andrzej',\n",
       " 'andy',\n",
       " 'anecdotal',\n",
       " 'anes',\n",
       " 'anette',\n",
       " 'anew',\n",
       " 'anexo',\n",
       " 'ang',\n",
       " 'angel',\n",
       " 'angela',\n",
       " 'angeles',\n",
       " 'angelic',\n",
       " 'angelica',\n",
       " 'angelides',\n",
       " 'angelika',\n",
       " 'angelo',\n",
       " 'anger',\n",
       " 'angering',\n",
       " 'angie',\n",
       " 'angle',\n",
       " 'angles',\n",
       " 'angry',\n",
       " 'angus',\n",
       " 'ani',\n",
       " 'animal',\n",
       " 'animated',\n",
       " 'anism',\n",
       " 'anita',\n",
       " 'anjali',\n",
       " 'anker',\n",
       " 'ankle',\n",
       " 'ann',\n",
       " 'anna',\n",
       " 'anne',\n",
       " 'annette',\n",
       " 'annex',\n",
       " 'annexation',\n",
       " 'annie',\n",
       " 'anniversary',\n",
       " 'annotated',\n",
       " 'announce',\n",
       " 'announced',\n",
       " 'announcement',\n",
       " 'announcements',\n",
       " 'announces',\n",
       " 'announcing',\n",
       " 'annoyance',\n",
       " 'annoyed',\n",
       " 'annoying',\n",
       " 'annu',\n",
       " 'annual',\n",
       " 'annually',\n",
       " 'annuities',\n",
       " 'annuity',\n",
       " 'anodes',\n",
       " 'anoop',\n",
       " 'anos',\n",
       " 'another',\n",
       " 'ans',\n",
       " 'ansi',\n",
       " 'answer',\n",
       " 'answered',\n",
       " 'answering',\n",
       " 'answers',\n",
       " 'ant',\n",
       " 'antarctic',\n",
       " 'anthony',\n",
       " 'anti',\n",
       " 'antic',\n",
       " 'anticipate',\n",
       " 'anticipated',\n",
       " 'anticipating',\n",
       " 'anticipation',\n",
       " 'anticline',\n",
       " 'antifraud',\n",
       " 'antigua',\n",
       " 'antipathy',\n",
       " 'antiquing',\n",
       " 'antivirus',\n",
       " 'antoinette',\n",
       " 'anton',\n",
       " 'antonino',\n",
       " 'antonio',\n",
       " 'anwar',\n",
       " 'anwr',\n",
       " 'anwser',\n",
       " 'anxious',\n",
       " 'anxiously',\n",
       " 'any',\n",
       " 'anybody',\n",
       " 'anyhow',\n",
       " 'anyhting',\n",
       " 'anymore',\n",
       " 'anyone',\n",
       " 'anythign',\n",
       " 'anything',\n",
       " 'anytime',\n",
       " 'anyway',\n",
       " 'anyways',\n",
       " 'anywhere',\n",
       " 'anzalone',\n",
       " 'ao',\n",
       " 'aobut',\n",
       " 'aoe',\n",
       " 'aol',\n",
       " 'aon',\n",
       " 'aos',\n",
       " 'ap',\n",
       " 'apache',\n",
       " 'aparicio',\n",
       " 'aparna',\n",
       " 'apart',\n",
       " 'apartment',\n",
       " 'apartments',\n",
       " 'apb',\n",
       " 'apes',\n",
       " 'aph',\n",
       " 'api',\n",
       " 'aploligize',\n",
       " 'apollo',\n",
       " 'apologies',\n",
       " 'apologise',\n",
       " 'apologize',\n",
       " 'apologized',\n",
       " 'apology',\n",
       " 'app',\n",
       " 'appalachia',\n",
       " 'appalachian',\n",
       " 'apparantly',\n",
       " 'apparatus',\n",
       " 'apparent',\n",
       " 'apparently',\n",
       " 'appe',\n",
       " 'appeal',\n",
       " 'appealable',\n",
       " 'appealed',\n",
       " 'appeals',\n",
       " 'appear',\n",
       " 'appearance',\n",
       " 'appeared',\n",
       " 'appearing',\n",
       " 'appears',\n",
       " 'appease',\n",
       " 'appellant',\n",
       " 'appellate',\n",
       " 'appendices',\n",
       " 'appending',\n",
       " 'appendix',\n",
       " 'appetit',\n",
       " 'appetite',\n",
       " 'applaud',\n",
       " 'apple',\n",
       " 'appleby',\n",
       " 'apples',\n",
       " 'appleton',\n",
       " 'appli',\n",
       " 'applicable',\n",
       " 'applicant',\n",
       " 'applicants',\n",
       " 'applicati',\n",
       " 'applicatio',\n",
       " 'application',\n",
       " 'applications',\n",
       " 'applied',\n",
       " 'applies',\n",
       " 'apply',\n",
       " 'applying',\n",
       " 'appoint',\n",
       " 'appointed',\n",
       " 'appointee',\n",
       " 'appointment',\n",
       " 'appointments',\n",
       " 'appollo',\n",
       " 'appology',\n",
       " 'appr',\n",
       " 'appraise',\n",
       " 'appre',\n",
       " 'apprec',\n",
       " 'appreciate',\n",
       " 'appreciated',\n",
       " 'appreciates',\n",
       " 'appreciation',\n",
       " 'appreciative',\n",
       " 'appreciatively',\n",
       " 'apprehension',\n",
       " 'apprehensive',\n",
       " 'apprised',\n",
       " 'appro',\n",
       " 'approach',\n",
       " 'approached',\n",
       " 'approaches',\n",
       " 'approaching',\n",
       " 'appropiate',\n",
       " 'appropri',\n",
       " 'appropriate',\n",
       " 'appropriately',\n",
       " 'appropriateness',\n",
       " 'appropriation',\n",
       " 'appropriations',\n",
       " 'approv',\n",
       " 'approval',\n",
       " 'approvals',\n",
       " 'approve',\n",
       " 'approved',\n",
       " 'approver',\n",
       " 'approves',\n",
       " 'approving',\n",
       " 'approx',\n",
       " 'approximate',\n",
       " 'approximately',\n",
       " 'approximating',\n",
       " 'apprx',\n",
       " 'apps',\n",
       " 'appt',\n",
       " 'appts',\n",
       " 'apr',\n",
       " 'april',\n",
       " 'apron',\n",
       " 'aps',\n",
       " 'apt',\n",
       " 'aptly',\n",
       " 'apts',\n",
       " 'apus',\n",
       " 'apx',\n",
       " 'apy',\n",
       " 'aq',\n",
       " 'aquila',\n",
       " 'aquilla',\n",
       " 'aquisition',\n",
       " 'ar',\n",
       " 'arana',\n",
       " 'arauco',\n",
       " 'arb',\n",
       " 'arbi',\n",
       " 'arbitrage',\n",
       " 'arbitrary',\n",
       " 'arbitration',\n",
       " 'arbitrator',\n",
       " 'arbitrators',\n",
       " 'arboretum',\n",
       " 'arbs',\n",
       " 'archaic',\n",
       " 'architects',\n",
       " 'architectural',\n",
       " 'architecture',\n",
       " 'archive',\n",
       " 'archived',\n",
       " 'archives',\n",
       " 'archiving',\n",
       " 'arco',\n",
       " 'arcos',\n",
       " 'arctic',\n",
       " 'arcy',\n",
       " 'ard',\n",
       " 'ards',\n",
       " 'are',\n",
       " 'area',\n",
       " 'areas',\n",
       " 'aren',\n",
       " ...]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sent):\n",
    "    '''Return the tokens of a sentence including punctuation.\n",
    "    >>> tokenize('Bob dropped the apple. Where is the apple?')\n",
    "    ['Bob', 'dropped', 'the', 'apple', '.', 'Where', 'is', 'the', 'apple', '?']\n",
    "    '''\n",
    "    return [x.strip() for x in re.findall(r\"[\\w']+|[.,!?]\", sent) if x.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(data):\n",
    "    questions = data.Context.apply(tokenize).values\n",
    "    answers = data.Utterance.apply(tokenize).values\n",
    "\n",
    "    #questions.apply(tokenize)\n",
    "    #answers.apply(tokenize)\n",
    "\n",
    "    data = [(q, a) for q, a in zip(questions, answers)]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = get_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21234"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Context</th>\n",
       "      <th>Utterance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i'm good why aren't you online in instant mess...</td>\n",
       "      <td>what are you doing for lunch ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>yep back in the saddle again for two weeks tha...</td>\n",
       "      <td>are you in today ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>goss and you can't go now because of some lag ...</td>\n",
       "      <td>what happened to the gambling trip ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>credit is not throwing you off . they did send...</td>\n",
       "      <td>marie can you please check on this and let ric...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>limor , the instructions below should help you...</td>\n",
       "      <td>put this in a safe place !</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Context  \\\n",
       "0  i'm good why aren't you online in instant mess...   \n",
       "1  yep back in the saddle again for two weeks tha...   \n",
       "2  goss and you can't go now because of some lag ...   \n",
       "3  credit is not throwing you off . they did send...   \n",
       "4  limor , the instructions below should help you...   \n",
       "\n",
       "                                           Utterance  \n",
       "0                     what are you doing for lunch ?  \n",
       "1                                 are you in today ?  \n",
       "2               what happened to the gambling trip ?  \n",
       "3  marie can you please check on this and let ric...  \n",
       "4                         put this in a safe place !  "
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "input1 = data['Context'].fillna(\"\")\n",
    "output1 = data['Utterance'].fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = list(map(pre_process,train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_counter = Counter()\n",
    "for question,answer in train:\n",
    "    for w in answer:\n",
    "        target_counter[w] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_word2idx = dict()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, word in enumerate(target_counter.most_common(len(vocab))):\n",
    "    target_word2idx[word[0]] = idx + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'UNK' not in target_word2idx:\n",
    "    target_word2idx['UNK'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_idx2word = dict([(idx, word) for word, idx in target_word2idx.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOVE_EMBEDDING_SIZE = 100\n",
    "HIDDEN_UNITS = 64\n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_texts = [question for question,answer in train]\n",
    "target_texts = [answer for question,answer in train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gensim.models.word2vec:collecting all words and their counts\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #10000, processed 478755 words, keeping 15117 word types\n",
      "INFO:gensim.models.word2vec:collected 17243 word types from a corpus of 733117 raw words and 15284 sentences\n",
      "INFO:gensim.models.word2vec:Loading a fresh vocabulary\n",
      "INFO:gensim.models.word2vec:effective_min_count=5 retains 7668 unique words (44% of original 17243, drops 9575)\n",
      "INFO:gensim.models.word2vec:effective_min_count=5 leaves 714267 word corpus (97% of original 733117, drops 18850)\n",
      "INFO:gensim.models.word2vec:deleting the raw counts dictionary of 17243 items\n",
      "INFO:gensim.models.word2vec:sample=0.001 downsamples 52 most-common words\n",
      "INFO:gensim.models.word2vec:downsampling leaves estimated 535756 word corpus (75.0% of prior 714267)\n",
      "INFO:gensim.models.base_any2vec:estimated required memory for 7668 words and 100 dimensions: 9968400 bytes\n",
      "INFO:gensim.models.word2vec:resetting layer weights\n",
      "INFO:gensim.models.base_any2vec:training model with 3 workers on 7668 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "DEBUG:gensim.models.base_any2vec:job loop exiting, total 74 jobs\n",
      "DEBUG:gensim.models.base_any2vec:worker exiting, processed 24 jobs\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "DEBUG:gensim.models.base_any2vec:worker exiting, processed 25 jobs\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "DEBUG:gensim.models.base_any2vec:worker exiting, processed 25 jobs\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.base_any2vec:EPOCH - 1 : training on 733117 raw words (535724 effective words) took 0.4s, 1340860 effective words/s\n",
      "DEBUG:gensim.models.base_any2vec:job loop exiting, total 74 jobs\n",
      "DEBUG:gensim.models.base_any2vec:worker exiting, processed 24 jobs\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "DEBUG:gensim.models.base_any2vec:worker exiting, processed 25 jobs\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "DEBUG:gensim.models.base_any2vec:worker exiting, processed 25 jobs\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.base_any2vec:EPOCH - 2 : training on 733117 raw words (535873 effective words) took 0.4s, 1397700 effective words/s\n",
      "DEBUG:gensim.models.base_any2vec:job loop exiting, total 74 jobs\n",
      "DEBUG:gensim.models.base_any2vec:worker exiting, processed 25 jobs\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "DEBUG:gensim.models.base_any2vec:worker exiting, processed 25 jobs\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "DEBUG:gensim.models.base_any2vec:worker exiting, processed 24 jobs\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.base_any2vec:EPOCH - 3 : training on 733117 raw words (535802 effective words) took 0.5s, 1112720 effective words/s\n",
      "DEBUG:gensim.models.base_any2vec:job loop exiting, total 74 jobs\n",
      "DEBUG:gensim.models.base_any2vec:worker exiting, processed 24 jobs\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "DEBUG:gensim.models.base_any2vec:worker exiting, processed 25 jobs\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "DEBUG:gensim.models.base_any2vec:worker exiting, processed 25 jobs\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.base_any2vec:EPOCH - 4 : training on 733117 raw words (536480 effective words) took 0.5s, 1129727 effective words/s\n",
      "DEBUG:gensim.models.base_any2vec:job loop exiting, total 74 jobs\n",
      "DEBUG:gensim.models.base_any2vec:worker exiting, processed 24 jobs\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 2 more threads\n",
      "DEBUG:gensim.models.base_any2vec:worker exiting, processed 24 jobs\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\n",
      "DEBUG:gensim.models.base_any2vec:worker exiting, processed 26 jobs\n",
      "INFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\n",
      "INFO:gensim.models.base_any2vec:EPOCH - 5 : training on 733117 raw words (535622 effective words) took 0.4s, 1265179 effective words/s\n",
      "INFO:gensim.models.base_any2vec:training on a 3665585 raw words (2679501 effective words) took 2.2s, 1222191 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size ::  7668\n",
      "['the', 'to', 'i', 'and', 'you', 'a', 'of', 'is', 'in', 'for']\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "sentences = input_texts\n",
    "model = word2vec.Word2Vec(sentences)\n",
    "vocab_size = len(model.wv.vocab)\n",
    "print(\"Vocab size :: \",vocab_size)\n",
    "print(model.wv.index2word[0:10])\n",
    "print(model.wv.index2word[vocab_size - 1:vocab_size-10])\n",
    "word2em = np.zeros((len(model.wv.vocab), 100))\n",
    "for i in range(len(model.wv.vocab)):\n",
    "    embedding_vector = model.wv[model.wv.index2word[i]]\n",
    "    if embedding_vector is not None:\n",
    "        word2em[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encoder_input_wids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:9: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "num_decoder_tokens = len(target_idx2word)+1\n",
    "input_texts_word2em = []\n",
    "encoder_max_seq_length = 0\n",
    "decoder_max_seq_length = 0\n",
    "for input_words, target_words in train:\n",
    "    encoder_input_wids = []\n",
    "    for w in input_words:\n",
    "        emb = np.zeros(shape=GLOVE_EMBEDDING_SIZE)\n",
    "        if w in word2em:\n",
    "            emb = word2em[w]\n",
    "        encoder_input_wids.append(emb)\n",
    "    input_texts_word2em.append(encoder_input_wids)\n",
    "    encoder_max_seq_length = max(len(encoder_input_wids), encoder_max_seq_length)\n",
    "    decoder_max_seq_length = max(len(target_words), decoder_max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = dict()\n",
    "context['num_decoder_tokens'] = num_decoder_tokens\n",
    "context['encoder_max_seq_length'] = encoder_max_seq_length\n",
    "context['decoder_max_seq_length'] = decoder_max_seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_encoder_seq_length = context['encoder_max_seq_length']\n",
    "max_decoder_seq_length = context['decoder_max_seq_length']\n",
    "num_decoder_tokens = context['num_decoder_tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context:  {'num_decoder_tokens': 8460, 'encoder_max_seq_length': 1154, 'decoder_max_seq_length': 30}\n"
     ]
    }
   ],
   "source": [
    "print(\"context: \",context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs = Input(shape=(None, GLOVE_EMBEDDING_SIZE), name='encoder_inputs')\n",
    "encoder_lstm1 = LSTM(units=HIDDEN_UNITS, return_state=True, name=\"encoder_lstm1\" , dropout=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_outputs, encoder_state_h, encoder_state_c = encoder_lstm1(encoder_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_states = [encoder_state_h, encoder_state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_inputs = Input(shape=(None, GLOVE_EMBEDDING_SIZE), name='decoder_inputs')\n",
    "decoder_lstm = LSTM(units=HIDDEN_UNITS, return_sequences=True, return_state=True, name='decoder_lstm', dropout=0.2)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax', name='decoder_dense')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_inputs (InputLayer)     (None, None, 100)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_inputs (InputLayer)     (None, None, 100)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder_lstm1 (LSTM)            [(None, 64), (None,  42240       encoder_inputs[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "decoder_lstm (LSTM)             [(None, None, 64), ( 42240       decoder_inputs[0][0]             \n",
      "                                                                 encoder_lstm1[0][1]              \n",
      "                                                                 encoder_lstm1[0][2]              \n",
      "__________________________________________________________________________________________________\n",
      "decoder_dense (Dense)           (None, None, 8460)   549900      decoder_lstm[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 634,380\n",
      "Trainable params: 634,380\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    " print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.0)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy',\n",
    "              metrics=['accuracy', precision_m])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain, Xtest, Ytrain, Ytest = train_test_split(input_texts_word2em, target_texts, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train data::  12227\n",
      "Length of test data::  3057\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of train data:: \", len(Xtrain))\n",
    "print(\"Length of test data:: \", len(Xtest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(input_word2em_data, output_text_data):\n",
    "    \n",
    "    num_batches = len(input_word2em_data) // BATCH_SIZE\n",
    "    print(\"context:: \\n\", context)\n",
    "    print(\"len of input data :: \", len(input_word2em_data))\n",
    "    print(\"num of batches :: \", num_batches)\n",
    "    \n",
    "    while True:\n",
    "        for batchIdx in range(0, num_batches):\n",
    "            \n",
    "            start = batchIdx * BATCH_SIZE\n",
    "            end = (batchIdx + 1) * BATCH_SIZE\n",
    "            \n",
    "            encoder_input_data_batch = pad_sequences(input_word2em_data[start:end], context['encoder_max_seq_length'])\n",
    "            decoder_target_data_batch = np.zeros(shape=(BATCH_SIZE, context['decoder_max_seq_length'], num_decoder_tokens))\n",
    "            decoder_input_data_batch = np.zeros(shape=(BATCH_SIZE, context['decoder_max_seq_length'], GLOVE_EMBEDDING_SIZE))\n",
    "            \n",
    "            for lineIdx, target_words in enumerate(output_text_data[start:end]):\n",
    "                for idx, w in enumerate(target_words):\n",
    "                    w2idx = target_word2idx['UNK']  # default UNK\n",
    "                    if w in target_word2idx:\n",
    "                        w2idx = target_word2idx[w]\n",
    "                    if w in word2em:\n",
    "                        decoder_input_data_batch[lineIdx, idx, :] = word2em[w]\n",
    "                    if idx > 0:\n",
    "                        decoder_target_data_batch[lineIdx, idx - 1, w2idx] = 1\n",
    "            yield [encoder_input_data_batch, decoder_input_data_batch], decoder_target_data_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = generate_batch(Xtrain, Ytrain)\n",
    "test_gen = generate_batch(Xtest, Ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_num_batches = len(Xtrain) // BATCH_SIZE\n",
    "test_num_batches = len(Xtest) // BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "context:: \n",
      " {'num_decoder_tokens': 8460, 'encoder_max_seq_length': 1154, 'decoder_max_seq_length': 30}\n",
      "len of input data ::  3057\n",
      "num of batches ::  47\n",
      "context:: \n",
      " {'num_decoder_tokens': 8460, 'encoder_max_seq_length': 1154, 'decoder_max_seq_length': 30}\n",
      "len of input data ::  12227\n",
      "num of batches ::  191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:23: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "191/191 [==============================] - 1194s 6s/step - loss: 2.7230 - acc: 0.0176 - precision_m: 0.0000e+00 - val_loss: 2.6244 - val_acc: 0.0163 - val_precision_m: 0.0000e+00\n",
      "Epoch 2/5\n",
      "102/191 [===============>..............] - ETA: 9:19 - loss: 2.6122 - acc: 0.0152 - precision_m: 0.0000e+00"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-361-2b8ae5431621>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m model.fit_generator(generator=train_gen, steps_per_epoch=train_num_batches,\n\u001b[1;32m      2\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m                     verbose=1, validation_data=test_gen, validation_steps=test_num_batches )\n\u001b[0m",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit_generator(generator=train_gen, steps_per_epoch=train_num_batches,\n",
    "                    epochs=5,\n",
    "                    verbose=1, validation_data=test_gen, validation_steps=test_num_batches )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(WEIGHT_FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = Model(encoder_inputs, encoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_state_inputs = [Input(shape=(HIDDEN_UNITS,)), Input(shape=(HIDDEN_UNITS,))]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_state_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model([decoder_inputs] + decoder_state_inputs, [decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(qa):\n",
    "    \n",
    "    question,answer = qa\n",
    "    w_q = [w for w in question if (len(w)!=1)|(w.isalpha())]\n",
    "    \n",
    "    w_a = [w for w in answer if (len(w)!=1)|(w.isalpha())]\n",
    "    return (w_q,w_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reply(input_text):\n",
    "    input_seq = []\n",
    "    input_emb = []\n",
    "    print(\"input text:: \\n\\n \", input_text)\n",
    "    # pre-processing only the input data \n",
    "    clean_input_text = input_text\n",
    "    for word in nltk.word_tokenize(clean_input_text.lower()):\n",
    "        emb = np.zeros(shape=GLOVE_EMBEDDING_SIZE)\n",
    "        if word in word2em:\n",
    "            emb = word2em[word]\n",
    "        input_emb.append(emb)\n",
    "    input_seq.append(input_emb)\n",
    "    input_seq = pad_sequences(input_seq, max_encoder_seq_length)\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    target_seq = np.zeros((1, 1, GLOVE_EMBEDDING_SIZE))\n",
    "    target_text = ''\n",
    "    target_text_len = 0\n",
    "    terminated = False\n",
    "    while not terminated:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "        print(\"output tokens shape  :: \\n\\n \", output_tokens.shape)\n",
    "        sample_token_idx = np.argmax(output_tokens[0, -1, :])\n",
    "        sample_word = target_idx2word[sample_token_idx]\n",
    "        target_text_len += 1\n",
    "        if sample_word != 'start' and sample_word != 'end':\n",
    "            print(\"sample word :: \", sample_word)\n",
    "            target_text += ' ' + sample_word\n",
    "        if sample_word == 'end' or target_text_len >= max_decoder_seq_length:\n",
    "            terminated = True\n",
    "        target_seq = np.zeros((1, 1, GLOVE_EMBEDDING_SIZE))\n",
    "        if sample_word in word2em:\n",
    "            target_seq[0, 0, :] = word2em[sample_word]\n",
    "        states_value = [h, c]\n",
    "    return target_text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input text:: \n",
      "\n",
      "  what are you doing for lunch\n",
      "output tokens shape  :: \n",
      "\n",
      "  (1, 1, 8460)\n",
      "sample word ::  you\n",
      "output tokens shape  :: \n",
      "\n",
      "  (1, 1, 8460)\n",
      "sample word ::  you\n",
      "output tokens shape  :: \n",
      "\n",
      "  (1, 1, 8460)\n",
      "sample word ::  the\n",
      "output tokens shape  :: \n",
      "\n",
      "  (1, 1, 8460)\n",
      "sample word ::  to\n",
      "output tokens shape  :: \n",
      "\n",
      "  (1, 1, 8460)\n",
      "sample word ::  the\n",
      "output tokens shape  :: \n",
      "\n",
      "  (1, 1, 8460)\n",
      "sample word ::  the\n",
      "output tokens shape  :: \n",
      "\n",
      "  (1, 1, 8460)\n",
      "sample word ::  the\n",
      "output tokens shape  :: \n",
      "\n",
      "  (1, 1, 8460)\n",
      "sample word ::  the\n",
      "output tokens shape  :: \n",
      "\n",
      "  (1, 1, 8460)\n",
      "sample word ::  the\n",
      "output tokens shape  :: \n",
      "\n",
      "  (1, 1, 8460)\n",
      "sample word ::  to\n",
      "output tokens shape  :: \n",
      "\n",
      "  (1, 1, 8460)\n",
      "sample word ::  to\n",
      "output tokens shape  :: \n",
      "\n",
      "  (1, 1, 8460)\n",
      "sample word ::  the\n",
      "output tokens shape  :: \n",
      "\n",
      "  (1, 1, 8460)\n",
      "sample word ::  the\n",
      "output tokens shape  :: \n",
      "\n",
      "  (1, 1, 8460)\n",
      "sample word ::  the\n",
      "output tokens shape  :: \n",
      "\n",
      "  (1, 1, 8460)\n",
      "sample word ::  the\n",
      "output tokens shape  :: \n",
      "\n",
      "  (1, 1, 8460)\n",
      "sample word ::  the\n",
      "output tokens shape  :: \n",
      "\n",
      "  (1, 1, 8460)\n",
      "sample word ::  the\n",
      "output tokens shape  :: \n",
      "\n",
      "  (1, 1, 8460)\n",
      "sample word ::  the\n",
      "output tokens shape  :: \n",
      "\n",
      "  (1, 1, 8460)\n",
      "sample word ::  thanks\n",
      "output tokens shape  :: \n",
      "\n",
      "  (1, 1, 8460)\n",
      "sample word ::  thanks\n",
      "output tokens shape  :: \n",
      "\n",
      "  (1, 1, 8460)\n",
      "sample word ::  thanks\n",
      "output tokens shape  :: \n",
      "\n",
      "  (1, 1, 8460)\n",
      "sample word ::  thanks\n",
      "output tokens shape  :: \n",
      "\n",
      "  (1, 1, 8460)\n",
      "sample word ::  thanks\n",
      "output tokens shape  :: \n",
      "\n",
      "  (1, 1, 8460)\n",
      "sample word ::  thanks\n",
      "output tokens shape  :: \n",
      "\n",
      "  (1, 1, 8460)\n",
      "sample word ::  thanks\n",
      "output tokens shape  :: \n",
      "\n",
      "  (1, 1, 8460)\n",
      "sample word ::  thanks\n",
      "output tokens shape  :: \n",
      "\n",
      "  (1, 1, 8460)\n",
      "sample word ::  thanks\n",
      "output tokens shape  :: \n",
      "\n",
      "  (1, 1, 8460)\n",
      "sample word ::  thanks\n",
      "output tokens shape  :: \n",
      "\n",
      "  (1, 1, 8460)\n",
      "sample word ::  thanks\n",
      "output tokens shape  :: \n",
      "\n",
      "  (1, 1, 8460)\n",
      "sample word ::  thanks\n",
      "you you the to the the the the the to to the the the the the the the thanks thanks thanks thanks thanks thanks thanks thanks thanks thanks thanks thanks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:9: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if __name__ == '__main__':\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:31: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n"
     ]
    }
   ],
   "source": [
    "print(reply('what are you doing for lunch'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/rachelchen/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_stopwords = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/rachelchen/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "APPO = {\n",
    "\"aren't\" : \"are not\",\n",
    "\"can't\" : \"cannot\",\n",
    "\"couldn't\" : \"could not\",\n",
    "\"didn't\" : \"did not\",\n",
    "\"doesn't\" : \"does not\",\n",
    "\"don't\" : \"do not\",\n",
    "\"hadn't\" : \"had not\",\n",
    "\"hasn't\" : \"has not\",\n",
    "\"haven't\" : \"have not\",\n",
    "\"he'd\" : \"he would\",\n",
    "\"he'll\" : \"he will\",\n",
    "\"he's\" : \"he is\",\n",
    "\"i'd\" : \"I would\",\n",
    "\"i'd\" : \"I had\",\n",
    "\"i'll\" : \"I will\",\n",
    "\"i'm\" : \"I am\",\n",
    "\"isn't\" : \"is not\",\n",
    "\"it's\" : \"it is\",\n",
    "\"it'll\":\"it will\",\n",
    "\"i've\" : \"I have\",\n",
    "\"let's\" : \"let us\",\n",
    "\"mightn't\" : \"might not\",\n",
    "\"mustn't\" : \"must not\",\n",
    "\"shan't\" : \"shall not\",\n",
    "\"she'd\" : \"she would\",\n",
    "\"she'll\" : \"she will\",\n",
    "\"she's\" : \"she is\",\n",
    "\"shouldn't\" : \"should not\",\n",
    "\"that's\" : \"that is\",\n",
    "\"there's\" : \"there is\",\n",
    "\"they'd\" : \"they would\",\n",
    "\"they'll\" : \"they will\",\n",
    "\"they're\" : \"they are\",\n",
    "\"they've\" : \"they have\",\n",
    "\"we'd\" : \"we would\",\n",
    "\"we're\" : \"we are\",\n",
    "\"weren't\" : \"were not\",\n",
    "\"we've\" : \"we have\",\n",
    "\"what'll\" : \"what will\",\n",
    "\"what're\" : \"what are\",\n",
    "\"what's\" : \"what is\",\n",
    "\"what've\" : \"what have\",\n",
    "\"where's\" : \"where is\",\n",
    "\"who'd\" : \"who would\",\n",
    "\"who'll\" : \"who will\",\n",
    "\"who're\" : \"who are\",\n",
    "\"who's\" : \"who is\",\n",
    "\"who've\" : \"who have\",\n",
    "\"won't\" : \"will not\",\n",
    "\"wouldn't\" : \"would not\",\n",
    "\"you'd\" : \"you would\",\n",
    "\"you'll\" : \"you will\",\n",
    "\"you're\" : \"you are\",\n",
    "\"you've\" : \"you have\",\n",
    "\"'re\": \" are\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'll\":\" will\",\n",
    "\"didn't\": \"did not\",\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "lem = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_recall(y, y_test, k=1):\n",
    "    num_examples = float(len(y))\n",
    "    num_correct = 0\n",
    "    for predictions, label in zip(y, y_test):\n",
    "        if label in predictions[:k]:\n",
    "            num_correct += 1\n",
    "    return num_correct/num_examples\n",
    "\n",
    "\n",
    "def test_precision_at_k(pred_opt, feed_dict, k, sess):\n",
    "    sims = pred_opt.eval(session=sess, feed_dict=feed_dict)\n",
    "    labels = range(0, len(sims))\n",
    "    for i, pred_vector in enumerate(sims):\n",
    "        sims[i] = [i[0] for i in sorted(enumerate(pred_vector), key=lambda x: x[1])][::-1]\n",
    "\n",
    "    recall_score = evaluate_recall(sims, labels, k)\n",
    "    return recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
